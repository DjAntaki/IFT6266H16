{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#handmaded_1layer_MLP\n",
    "\n",
    "Still some bugs when batch_size != 1..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip, pickle\n",
    "f = gzip.open('data/mnist.pkl.gz')\n",
    "\n",
    "data = pickle.load(f)\n",
    "\n",
    "train = data[0][0]\n",
    "train_y =data[0][1]\n",
    "\n",
    "valid = data[1][0]\n",
    "valid_y = data[1][1]\n",
    "\n",
    "test = data[2][0]\n",
    "test_y = data[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.   4.   5.   6.   0.   0.2  0. ]\n",
      "[ 1.  1.  1.  1.  0.  1.  0.]\n",
      "[ 0.33333333  0.33333333  0.33333333]\n"
     ]
    }
   ],
   "source": [
    "#Activation functions and their derivatives. maded to take a vector in input.\n",
    "def rect(v):\n",
    "    return np.maximum(np.zeros(v.shape),v);\n",
    "\n",
    "def rect_prime(v):\n",
    "    tmp = np.zeros(v.shape[0])\n",
    "    for i in xrange(v.shape[0]):\n",
    "        if( v[i] < 0):\n",
    "            tmp[i] = 0\n",
    "        else:\n",
    "            tmp[i] = 1\n",
    "    return tmp\n",
    "\n",
    "rect.prime = rect_prime\n",
    "\n",
    "\n",
    "def softmax(a):\n",
    "    \"\"\" Stable implementation on softmax function \"\"\"\n",
    "    a_max = np.max(a)\n",
    "   \n",
    "    n = 0 # normalisation_factor\n",
    "    s = np.zeros(len(a))\n",
    "   \n",
    "    for i, x in enumerate(a):\n",
    "        s[i] = np.exp(x-a_max)\n",
    "        n += s[i]\n",
    "       \n",
    "    return 1.0/n * s\n",
    "\n",
    "def linear(a):\n",
    "    return a\n",
    "\n",
    "def linear_prime(a):\n",
    "    return np.ones(a.shape[0])\n",
    "\n",
    "def sigmoid(a): #needs to be vectorized\n",
    "    return 1.0/(1+np.exp(-a))\n",
    "\n",
    "def sigmoid_prime(a): #needs to be vectorized\n",
    "    x = sigmoid(a)\n",
    "    return x*(1-x)\n",
    "\n",
    "sigmoid.prime = sigmoid_prime\n",
    "def tanh(a):#needs to be vectorized\n",
    "    return (np.exp(2*a) - 1)/(np.exp(2*a) + 1)\n",
    "\n",
    "#needs to be vectorized\n",
    "tanh.prime = lambda a : 1.0 - (tanh(a))**2\n",
    "\n",
    "#Costs functions for supervised learning\n",
    "def L1(x,y):\n",
    "    pass\n",
    "\n",
    "def L2(x,y):\n",
    "    assert x.size == y.size\n",
    "    return np.sum(np.abs(x-y))\n",
    "\n",
    "def EM(x,y):\n",
    "    return -1.0*np.log(x[np.where(y==1)])\n",
    "\n",
    "#utility\n",
    "def onehot(m, y):\n",
    "    result = np.zeros(m)\n",
    "    result[y] = 1\n",
    "    return result\n",
    "\n",
    "def chunker(seq, size):\n",
    "    #Taken from http://stackoverflow.com/questions/434287/what-is-the-most-pythonic-way-to-iterate-over-a-list-in-chunks\n",
    "    return (seq[pos:pos + size] for pos in xrange(0, len(seq), size))\n",
    "\n",
    "def return_if_null(a,b):\n",
    "    \"\"\" if a is None then return b else do return a\"\"\"\n",
    "    if a is None:\n",
    "        return b\n",
    "    return a\n",
    "\n",
    "print rect(np.array([3,4,5,6,-2, 0.2, -9]))\n",
    "print rect.prime(np.array([3,4,5,6,-2, 0.2, -9]))\n",
    "print softmax(np.array([1,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\" 'handmade' 1 hidden layer MLP\"\"\"\n",
    "    def __init__(self,nb_input,nb_hidden,nb_output,hidden_func=rect,output_func=softmax, cost_func=EM\n",
    "                 ,init_dist= lambda i,j: np.random.uniform(low=-1.0,high=1.0) ):\n",
    "        \n",
    "        self.W1= np.random.uniform(high=1,low=1,size= (nb_input, nb_hidden))\n",
    "        self.W2= np.random.uniform(high=1,low=1,size= (nb_hidden,nb_output))\n",
    "        #self.W2= np.fromfunction(init_dist, (nb_hidden, nb_output), dtype=np.float)\n",
    "        self.b1= np.zeros(nb_hidden,dtype=np.float)\n",
    "        self.b2= np.zeros(nb_output,dtype=np.float)\n",
    "        self.hidden_func = hidden_func\n",
    "        self.output_func = output_func\n",
    "        self.cost_func = cost_func\n",
    "        \n",
    "    def train(self,data,data_labels, batch_size = None,nb_epoch=1):\n",
    "        if batch_size == None:\n",
    "            batch_size = len(data)\n",
    "        \n",
    "        for i in range(nb_epoch):\n",
    "            if batch_size == 1:\n",
    "\n",
    "                for i in range(len(data)):\n",
    "                    grad_b1, grad_W1, grad_b2, grad_W2 = self.backprop(data[i],data_labels[i])\n",
    "                    self.update_parameters(grad_b1, grad_W1, grad_b2, grad_W2)\n",
    "\n",
    "                \n",
    "            else :\n",
    "                grad_b1 = np.zeros((batch_size,self.b1.shape[0]),dtype=np.float)\n",
    "                grad_b2 = np.zeros((batch_size,self.b2.shape[0]),dtype=np.float)\n",
    "                grad_W1 = np.zeros((batch_size,self.W1.shape[0],self.W1.shape[1]),dtype=np.float)\n",
    "                grad_W2 = np.zeros((batch_size,self.W2.shape[0],self.W2.shape[1]),dtype=np.float)\n",
    "\n",
    "                for i,j in zip(chunker(data,batch_size),chunker(data_labels,batch_size)):                \n",
    "                    #  print(i,j)\n",
    "                    y=0\n",
    "                    #print(i)\n",
    "                    for x in i:\n",
    "                        grad_b1[y], grad_W1[y], grad_b2[y], grad_W2[y] = self.backprop(x,j)\n",
    "                        y += 1\n",
    "                    \n",
    "                    if len(i) == batch_size:\n",
    "                        self.update_parameters(np.mean(grad_b1), np.mean(grad_W1), np.mean(grad_b2), np.mean(grad_W2))\n",
    "                    else :\n",
    "                        b = len(i)\n",
    "                        assert b == y\n",
    "                        self.update_parameters(np.mean(grad_b1[:b]), np.mean(grad_W1[:b]), np.mean(grad_b2[:b]), np.mean(grad_W2[:b]))\n",
    "\n",
    "\n",
    "    def compute_predictions(self,data):   \n",
    "        return np.array([self.compute_prediction(data[i]) for i in range(len(data))])\n",
    "\n",
    "    def compute_prediction(self,x):\n",
    "        return np.argmax(self.frontprop(x,self.b1,self.W1,self.b2,self.W2)[3])\n",
    "        \n",
    "    def frontprop(self,x,b1,W1,b2,W2):\n",
    "        ha = np.dot(x,W1) + b1\n",
    "        #ha = np.dot(W1,x) + b1\n",
    "        \n",
    "        hs = self.hidden_func(ha)         \n",
    "        oa = np.dot(hs,W2) + b2\n",
    "        os = self.output_func(oa)\n",
    "        #np.sum(-self.t*np.log(self.y)\n",
    "        #L = -1.0*np.log(os[y])\n",
    "        return ha,hs,oa,os\n",
    "        \n",
    "    def backprop(self,x,y):\n",
    "        \"\"\"\n",
    "        Computes frontprop and backprop\n",
    "        self.grad_b2 = grad_oa   \n",
    "        self.grad_b1 = self.grad_ha\n",
    "        \"\"\"\n",
    "        \n",
    "        ha,hs,oa,os = self.frontprop(x,self.b1,self.W1,self.b2,self.W2)\n",
    "        #print(ha.shape,hs.shape,oa.shape,os.shape)\n",
    "    \n",
    "        grad_oa = os - onehot(self.b2.shape[0], y)\n",
    "        #grad_W2 = np.outer(grad_oa,hs)\n",
    "        grad_W2 = np.outer(hs,grad_oa)\n",
    "        #grad_hs = np.dot(self.W2, grad_oa )\n",
    "        grad_hs = np.dot(self.W2, grad_oa )\n",
    "        \n",
    "        grad_ha = np.multiply(self.hidden_func.prime(ha), grad_hs )\n",
    "        #grad_W1 = np.outer(x,grad_ha)\n",
    "        grad_W1 = np.outer(x,grad_ha)\n",
    "        \n",
    "        #grad_x = np.dot( self.W1.T, grad_ha)\n",
    "        #  print(grad_W1.shape, grad_ha.shape, grad_hs.shape,grad_W2.shape, grad_oa.shape)\n",
    "        return grad_ha, grad_W1, grad_oa, grad_W2\n",
    "    \n",
    "    def update_parameters(self,grad_b1,grad_W1,grad_b2,grad_W2,learning_rate=0.1):\n",
    "        #print(self.b1,grad_b1)\n",
    "        self.b1 -= learning_rate * grad_b1\n",
    "        self.W1 -= learning_rate * grad_W1\n",
    "        self.b2 -= learning_rate * grad_b2\n",
    "        self.W2 -= learning_rate * grad_W2\n",
    "    \n",
    "    def printparameters(self):\n",
    "        print(\"b1 : \"+str(self.b1)+\" W1 : \"+str(self.W1))\n",
    "        print(\"b2 : \"+str(self.b2)+\" W2 : \"+str(self.W2))     \n",
    "    \n",
    "    def grad_diff(self, x,y, epsi = 0.000001):\n",
    "        \"\"\"Gradient finite difference. for debugging purposes.\"\"\"\n",
    "        grad_b1 = np.zeros(self.b1.shape[0])\n",
    "        grad_b2 = np.zeros(self.b2.shape[0])\n",
    "        grad_W1 = np.zeros(self.W1.shape)\n",
    "        grad_W2 = np.zeros(self.W2.shape)\n",
    "\n",
    "\n",
    "        yy = onehot(self.b2.shape[0], y)\n",
    "\n",
    "        cost = self.cost_func(self.frontprop(x,self.b1,self.W1,self.b2,self.W2)[3],yy)\n",
    "        gb1,gw1, gb2, gw2 = self.backprop(x,y)\n",
    "        \n",
    "        # compare b1\n",
    "        b1 = np.array(self.b1)\n",
    "        for i in xrange(self.b1.shape[0]):  \n",
    "            b1[i] -= epsi\n",
    "            os2 = self.frontprop(x,b1,self.W1,self.b2,self.W2)[3]\n",
    "            cost2 = self.cost_func(os2,yy)\n",
    "\n",
    "            grad_b1[i] = (cost - cost2)/epsi\n",
    "            b1[i] += epsi\n",
    "\n",
    "        print \"\\nValeur grad_b1\", gb1\n",
    "        print \"\\nValeur par différence finit\", grad_b1\n",
    "\n",
    "        # compare b2\n",
    "        b2 = np.array(self.b2)\n",
    "        for i in xrange(self.b2.shape[0]):  \n",
    "            b2[i] -= epsi\n",
    "            os2 = self.frontprop(x,self.b1,self.W1,b2,self.W2)[3]\n",
    "            cost2 = self.cost_func(os2,yy)\n",
    "            grad_b2[i] = (cost - cost2)/epsi\n",
    "            b2[i] += epsi\n",
    "\n",
    "        print \"\\nValeur grad_b2\", gb2\n",
    "        print \"\\nValeur par différence finit\", grad_b2\n",
    "\n",
    "        # compare W1\n",
    "        W1 = np.array(self.W1)\n",
    "        for i,j in product(xrange(self.W1.shape[0]), xrange(self.W1.shape[1])):\n",
    "            W1[i][j] -= epsi\n",
    "            os2 = self.frontprop(x,self.b1,W1,self.b2,self.W2)[3]\n",
    "            cost2 = self.cost_func(os2,yy)\n",
    "            grad_W1[i][j] = (cost - cost2)/epsi\n",
    "            W1[i][j] += epsi\n",
    "\n",
    "        print \"\\nValeur grad_W1\", gw1\n",
    "        print \"\\nValeur par différence finit\", grad_W1\n",
    "\n",
    "        # compare W2\n",
    "        W2 = np.array(self.W2)\n",
    "        for i,j in product(xrange(W2.shape[0]), xrange(W2.shape[1])):\n",
    "            W2[i][j] -= epsi\n",
    "            os2 = self.frontprop(x,self.b1,self.W1,self.b2,W2)[3]\n",
    "            cost2 = self.cost_func(os2,yy)\n",
    "            grad_W2[i][j] = (cost - cost2)/epsi\n",
    "            W2[i][j] += epsi\n",
    "\n",
    "        print \"\\nValeur grad_W2\", gw2\n",
    "        print \"\\nValeur par différence finit\",grad_W2\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_input = len(train[0])\n",
    "nb_output = 10\n",
    "nb_hidden = 200\n",
    "model = MLP(nb_input,nb_hidden,nb_output)\n",
    "model.train(train,train_y,1)\n",
    "\n",
    "#def __init__(self,nb_input,nb_hidden,nb_output,hidden_layer_func=rect,output_func=softmax, cost_func=L2                 ,init_dist= lambda i,j: np.random.uniform(low=-1.0,high=1.0) ):\n",
    "\n",
    "        \n",
    "# def train(self,data, batch_size = None,nb_epoch=1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.grad_diff(test[0],test_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 0 0 0]\n",
      "[7 2 1 ..., 4 5 6]\n",
      "(\"tx d'erreurs\", 0.902)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_prediction = model.compute_predictions(test)\n",
    "print(test_prediction)\n",
    "print(test_y)\n",
    "errors = 0\n",
    "nb_test = len(test_y)\n",
    "for i in range(nb_test):\n",
    "    if test_prediction[i] != test_y[i]:\n",
    "        errors += 1\n",
    "print('tx d\\'erreurs',float(errors)/nb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model.W2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Retourne la valeur de L en fonction des matrices données\n",
    "class fprop:\n",
    "    def __init__(self, W1,W2,b1,b2,x,y):\n",
    "        self.W1=W1\n",
    "        self.W2=W2\n",
    "        self.b1=b1\n",
    "        self.b2=b2\n",
    "        self.x=x\n",
    "        self.y=y       \n",
    "        self.m = b2.shape[0]\n",
    "        self.dh = b1.shape[0]\n",
    "        \n",
    "    def compute(self):\n",
    "        self.ha = np.dot(self.W1, self.x) + self.b1\n",
    "        self.hs = rect(self.ha)\n",
    "        self.os = softmax(np.dot(self.W2, self.hs) + self.b2)\n",
    "        self.L = -1.0*np.log(self.os[self.y])\n",
    "        return self.L\n",
    "    \n",
    "    def compute_predictions(self, data):\n",
    "        self.ha = np.dot(self.W1, data) + self.b1\n",
    "        self.hs = rect(self.ha)\n",
    "        self.os = softmax(np.dot(self.W2, self.hs) + self.b2)\n",
    "        return np.argmax(self.os)\n",
    "    \n",
    "#calcul des gradients\n",
    "class bprop:\n",
    "    def __init__(self, fprop):\n",
    "        self.fprop = fprop\n",
    "        \n",
    "        grad_oa = fprop.os - onehot(fprop.m, fprop.y)\n",
    "        self.grad_b2 = grad_oa   \n",
    "        self.grad_W2 = np.outer( grad_oa, fprop.hs)\n",
    "        self.grad_hs = np.dot(fprop.W2.T, grad_oa )\n",
    "        self.grad_ha = np.multiply(rectPrime(fprop.ha), self.grad_hs )\n",
    "        self.grad_b1 = self.grad_ha\n",
    "        self.grad_W1 = np.outer( self.grad_ha, fprop.x)\n",
    "        self.grad_x = np.dot( fprop.W1.T, self.grad_ha)\n",
    "        #print(grad_ha.shape, grad_hs.shape, grad_oa.shape)\n",
    "\n",
    "        \n",
    "\n",
    "# calcul du gradient par différence finit\n",
    "def gradDiff(bprop):\n",
    "    epsi = 0.000001\n",
    "    \n",
    "    grad_b1 = np.zeros(bprop.fprop.b1.shape[0])\n",
    "    grad_b2 = np.zeros(bprop.fprop.b2.shape[0])\n",
    "    grad_W1 = np.zeros(bprop.fprop.W1.shape)\n",
    "    grad_W2 = np.zeros(bprop.fprop.W2.shape)\n",
    "\n",
    "    \n",
    "    # compare b1\n",
    "    L = bprop.fprop.L\n",
    "    for i in xrange(bprop.fprop.b1.shape[0]):  \n",
    "        bprop.fprop.b1[i] -= epsi\n",
    "        bprop.fprop.compute()\n",
    "        grad_b1[i] = (L - bprop.fprop.L)/epsi\n",
    "        bprop.fprop.b1[i] += epsi\n",
    "        \n",
    "    print \"\\nValeur grad_b1\"\n",
    "    print bprop.grad_b1\n",
    "    print \"Valeur par différence finit\"\n",
    "    print grad_b1\n",
    "   \n",
    "    # compare b2\n",
    "    bprop.fprop.compute()\n",
    "    L = bprop.fprop.L\n",
    "    for i in xrange(bprop.fprop.b2.shape[0]):  \n",
    "        bprop.fprop.b2[i] -= epsi\n",
    "        bprop.fprop.compute()\n",
    "        grad_b2[i] = (L - bprop.fprop.L)/epsi\n",
    "        bprop.fprop.b2[i] += epsi\n",
    "\n",
    "    print \"\\nValeur grad_b2\"\n",
    "    print bprop.grad_b2\n",
    "    print \"Valeur par différence finit\"\n",
    "    print grad_b2\n",
    "        \n",
    "    \n",
    "    # compare W1\n",
    "    bprop.fprop.compute()\n",
    "    L = bprop.fprop.L\n",
    "    for i in xrange(bprop.fprop.W1.shape[0]):\n",
    "        for j in xrange(bprop.fprop.W1.shape[1]):\n",
    "            bprop.fprop.W1[i][j] -= epsi\n",
    "            bprop.fprop.compute()\n",
    "            grad_W1[i][j] = (L - bprop.fprop.L)/epsi\n",
    "            bprop.fprop.W1[i][j] += epsi\n",
    "            \n",
    "    print \"\\nValeur grad_W1\"\n",
    "    print bprop.grad_W1\n",
    "    print \"Valeur par différence finit\"\n",
    "    print grad_W1\n",
    "\n",
    "    \n",
    "    # compare W2\n",
    "    bprop.fprop.compute()\n",
    "    L = bprop.fprop.L\n",
    "    for i in xrange(bprop.fprop.W2.shape[0]):\n",
    "        for j in xrange(bprop.fprop.W2.shape[1]):\n",
    "            bprop.fprop.W2[i][j] -= epsi\n",
    "            bprop.fprop.compute()\n",
    "            grad_W2[i][j] = (L - bprop.fprop.L)/epsi\n",
    "            bprop.fprop.W2[i][j] += epsi\n",
    "            \n",
    "    bprop.fprop.compute()\n",
    "    \n",
    "    print \"\\nValeur grad_W2\"\n",
    "    print bprop.grad_W2\n",
    "    print \"Valeur par différence finit\"\n",
    "    print grad_W2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

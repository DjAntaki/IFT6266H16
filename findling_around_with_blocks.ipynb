{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# findling around with blocks\n",
    "\n",
    "This is my test ground to get to know the Blocks framework for deep learning and make keep some basics examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------\n",
      "BEFORE FIRST EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 0\n",
      "\t received_first_batch: False\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 0:\n",
      "\t test_cost_with_regularization: 2.34214115531\n",
      "\t test_misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 1\n",
      "\t iterations_done: 235\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 235:\n",
      "\t test_cost_with_regularization: 0.655349486793\n",
      "\t test_misclassificationrate_apply_error_rate: 0.993630573248\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 2\n",
      "\t iterations_done: 470\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 470:\n",
      "\t test_cost_with_regularization: 0.570589023689\n",
      "\t test_misclassificationrate_apply_error_rate: 0.993630573248\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 3\n",
      "\t iterations_done: 705\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 705:\n",
      "\t test_cost_with_regularization: 0.547092178475\n",
      "\t test_misclassificationrate_apply_error_rate: 0.993630573248\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 4\n",
      "\t iterations_done: 940\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 940:\n",
      "\t test_cost_with_regularization: 0.535774715088\n",
      "\t test_misclassificationrate_apply_error_rate: 0.993630573248\n",
      "\t training_finish_requested: True\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "TRAINING HAS BEEN FINISHED:\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 4\n",
      "\t iterations_done: 940\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 940:\n",
      "\t test_cost_with_regularization: 0.535774715088\n",
      "\t test_misclassificationrate_apply_error_rate: 0.993630573248\n",
      "\t training_finish_requested: True\n",
      "\t training_finished: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#MNIST example of blocks : https://blocks.readthedocs.org/en/latest/tutorial.html\n",
    "from theano import tensor\n",
    "x = tensor.matrix('features')\n",
    "\n",
    "from blocks.bricks import Linear, Rectifier, Softmax\n",
    "input_to_hidden = Linear(name='input_to_hidden', input_dim=784, output_dim=100)\n",
    "h = Rectifier().apply(input_to_hidden.apply(x))\n",
    "hidden_to_output = Linear(name='hidden_to_output', input_dim=100, output_dim=10)\n",
    "y_hat = Softmax().apply(hidden_to_output.apply(h))\n",
    "\n",
    "y = tensor.lmatrix('targets')\n",
    "from blocks.bricks.cost import CategoricalCrossEntropy,SquaredError\n",
    "cost = CategoricalCrossEntropy().apply(y.flatten(), y_hat)\n",
    "\n",
    "\n",
    "from blocks.bricks import WEIGHT\n",
    "from blocks.graph import ComputationGraph\n",
    "from blocks.filter import VariableFilter\n",
    "cg = ComputationGraph(cost)\n",
    "W1, W2 = VariableFilter(roles=[WEIGHT])(cg.variables)\n",
    "cost = cost + 0.005 * (W1 ** 2).sum() + 0.005 * (W2 ** 2).sum()\n",
    "cost.name = 'cost_with_regularization'\n",
    "\n",
    "\n",
    "#Short for above\n",
    "from blocks.bricks import MLP\n",
    "x = tensor.matrix('features')\n",
    "mlp = MLP(activations=[Rectifier(), Softmax()], dims=[784, 400, 10]).apply(x)\n",
    "\n",
    "from blocks.initialization import IsotropicGaussian, Constant\n",
    "input_to_hidden.weights_init = hidden_to_output.weights_init = IsotropicGaussian(0.01)\n",
    "input_to_hidden.biases_init = hidden_to_output.biases_init = Constant(0)\n",
    "input_to_hidden.initialize()\n",
    "hidden_to_output.initialize()\n",
    "\n",
    "W1.get_value() \n",
    "\n",
    "from fuel.datasets import MNIST\n",
    "mnist = MNIST((\"train\",))\n",
    "\n",
    "\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme\n",
    "from fuel.transformers import Flatten\n",
    "data_stream = Flatten(DataStream.default_stream(\n",
    "    mnist,\n",
    "    iteration_scheme=SequentialScheme(mnist.num_examples, batch_size=256)))\n",
    "\n",
    "from blocks.algorithms import GradientDescent, Scale\n",
    "algorithm = GradientDescent(cost=cost, parameters=cg.parameters,\n",
    "                             step_rule=Scale(learning_rate=0.1))\n",
    "\n",
    "\n",
    "\n",
    "mnist_test = MNIST((\"test\",))\n",
    "data_stream_test = Flatten(DataStream.default_stream(\n",
    "    mnist_test,\n",
    "    iteration_scheme=SequentialScheme(\n",
    "       mnist_test.num_examples, batch_size=64)))\n",
    "\n",
    "\n",
    "\n",
    "from blocks.extensions.monitoring import DataStreamMonitoring,DataStreamMonitoring\n",
    "classification_error= MisclassificationRate().apply(np.argmax(y_hat),y)\n",
    "monitor = DataStreamMonitoring(\n",
    "    variables=[cost,classification_error], data_stream=data_stream_test, prefix=\"test\")\n",
    "\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.extensions import FinishAfter, Printing\n",
    "main_loop = MainLoop(data_stream=data_stream, algorithm=algorithm,\n",
    "                     extensions=[monitor, FinishAfter(after_n_epochs=4), Printing()])\n",
    "main_loop.run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp_apply_output\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "BEFORE FIRST EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 0\n",
      "\t received_first_batch: False\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 0:\n",
      "\t test_categoricalcrossentropy_apply_cost: 28.1514429187\n",
      "\t test_misclassificationrate_apply_error_rate: 0.891719745223\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 1\n",
      "\t iterations_done: 235\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 235:\n",
      "\t test_categoricalcrossentropy_apply_cost: 0.274219271408\n",
      "\t test_misclassificationrate_apply_error_rate: 0.927448248408\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 2\n",
      "\t iterations_done: 470\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 470:\n",
      "\t test_categoricalcrossentropy_apply_cost: 0.216756095763\n",
      "\t test_misclassificationrate_apply_error_rate: 0.927945859873\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 3\n",
      "\t iterations_done: 705\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 705:\n",
      "\t test_categoricalcrossentropy_apply_cost: 0.191544942296\n",
      "\t test_misclassificationrate_apply_error_rate: 0.928045382166\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 4\n",
      "\t iterations_done: 940\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 940:\n",
      "\t test_categoricalcrossentropy_apply_cost: 0.174388112169\n",
      "\t test_misclassificationrate_apply_error_rate: 0.928343949045\n",
      "\t training_finish_requested: True\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "TRAINING HAS BEEN FINISHED:\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 4\n",
      "\t iterations_done: 940\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 940:\n",
      "\t test_categoricalcrossentropy_apply_cost: 0.174388112169\n",
      "\t test_misclassificationrate_apply_error_rate: 0.928343949045\n",
      "\t training_finish_requested: True\n",
      "\t training_finished: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#I started from the example above and added some couple of things\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "#Importing dataset\n",
    "from fuel.datasets import MNIST\n",
    "mnist = MNIST((\"train\",))\n",
    "\n",
    "#Data Stream\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme\n",
    "from fuel.transformers import Flatten\n",
    "data_stream = Flatten(DataStream.default_stream(\n",
    "    mnist,\n",
    "    iteration_scheme=SequentialScheme(mnist.num_examples, batch_size=256)))\n",
    "\n",
    "#Model\n",
    "from blocks.bricks import Tanh,Linear, Rectifier, Softmax, MLP, Logistic\n",
    "from blocks.initialization import IsotropicGaussian, Constant\n",
    "from theano import tensor\n",
    "\n",
    "x = tensor.matrix('features')\n",
    "y1 = tensor.lmatrix(name='targets')\n",
    "\n",
    "#mlp = MLP(activations=[Logistic(name='sigmoid_0'),\n",
    "#          Rectifier(name='rect1'),Softmax(name='softmax_0')], dims=[784, 300, 200,10],\n",
    "#          weights_init=IsotropicGaussian(), biases_init=Constant(0.01))\n",
    "\n",
    "mlp = MLP(activations=[Rectifier(name='rect0'),Logistic(name='sigmoid_1'),Softmax(name='softmax_0')], dims=[784, 300, 300, 10],\n",
    "          weights_init=IsotropicGaussian(), biases_init=Constant(0.01))\n",
    "\n",
    "\n",
    "y = mlp.apply(x)\n",
    "print(y)\n",
    "mlp.push_initialization_config()\n",
    "mlp.children[0].weights_init = Constant(0.01)\n",
    "mlp.initialize()\n",
    "#print(mlp.children[0].parameters[0].get_value()) \n",
    "\n",
    "\n",
    "#Cost\n",
    "from blocks.graph import ComputationGraph\n",
    "from blocks.bricks.cost import CategoricalCrossEntropy,SquaredError,MisclassificationRate\n",
    "from blocks.algorithms import Scale,GradientDescent\n",
    "cost = CategoricalCrossEntropy().apply(y1.flatten(),y)\n",
    "algorithm = GradientDescent(cost=cost, parameters=ComputationGraph(cost).parameters,\n",
    "                             step_rule=Scale(learning_rate=0.005))\n",
    "\n",
    "\n",
    "#data stream test\n",
    "mnist_test = MNIST((\"test\",))\n",
    "data_stream_test = Flatten(DataStream.default_stream(\n",
    "    mnist_test,\n",
    "    iteration_scheme=SequentialScheme(\n",
    "       mnist_test.num_examples, batch_size=64)))\n",
    "\n",
    "\n",
    "#Monitor\n",
    "from blocks.extensions.monitoring import DataStreamMonitoring\n",
    "classification_error= MisclassificationRate().apply(np.argmax(y1),y)\n",
    "monitor = DataStreamMonitoring(\n",
    "    variables=[cost,classification_error], data_stream=data_stream_test, prefix=\"test\")\n",
    "\n",
    "#MainLoop\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.extensions import FinishAfter, Printing\n",
    "main_loop = MainLoop(algorithm,data_stream=data_stream, model=mlp,\n",
    "                     extensions=[monitor, FinishAfter(after_n_epochs=4), Printing()])\n",
    "main_loop.run()\n",
    "\n",
    "#%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Results\n",
    "\n",
    "batch_size, activation, dims, cost_used, learning_rate, epoch,  cost_with_regularization, misclassificationrate\n",
    "\n",
    "64,[Logistic(), Rectifier(),Softmax()],[784, 300, 200,10], CategoricalCrossEntropy, 0.05, 4, 0.165451308803, 0.927249203822\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from itertools import permutations,product\n",
    "\n",
    "\n",
    "for m,j,h in product([Logistic, Rectifier],[Logistic, Rectifier],[Logistic, Rectifier]):\n",
    "    print(m,j,x)\n",
    "    activation = [m(name=str(type(m))+'0'),j(name=str(type(j))+'1'),h(name=str(type(h))+'2'),Softmax(name='softmax_4')]\n",
    "    print(activation)\n",
    "    mlp = MLP(activations=activation, dims=[784,500,300,100,10],\n",
    "          weights_init=IsotropicGaussian(), biases_init=Constant(0.01))\n",
    "\n",
    "    try_model_on_mnist(mlp)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32m/home/ntak/.local/lib/python2.7/site-packages/blocks/bricks/__init__.py\u001b[0m(739)\u001b[0;36m_push_allocation_config\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m    738 \u001b[1;33m        \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_transformations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m--> 739 \u001b[1;33m            \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    740 \u001b[1;33m        \u001b[1;32mfor\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> self.dims\n",
      "[784, 300, 200, 10]\n",
      "ipdb> self.linear_transformations\n",
      "[<blocks.bricks.Linear object at 0x7fb189943dd0: name=linear_0>, <blocks.bricks.Linear object at 0x7fb18c716a90: name=linear_1>, <blocks.bricks.Linear object at 0x7fb189943e50: name=linear_2>, <blocks.bricks.Linear object at 0x7fb189943c90: name=linear_3>]\n"
     ]
    }
   ],
   "source": [
    "%debug\n",
    "print(list(permutations([Logistic, Rectifier],r=3)))\n",
    "print(Logistic)\n",
    "print(list(permutations([1,2,3],3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Importing dataset\n",
    "from fuel.datasets import MNIST\n",
    "mnist = MNIST((\"train\",))\n",
    "mnist_test = MNIST((\"test\",))\n",
    "\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme\n",
    "from fuel.transformers import Flatten\n",
    "\n",
    "from blocks.graph import ComputationGraph\n",
    "from blocks.bricks.cost import CategoricalCrossEntropy,SquaredError,MisclassificationRate\n",
    "from blocks.algorithms import Scale,GradientDescent\n",
    "\n",
    "from blocks.bricks import Tanh,Linear, Rectifier, Softmax, MLP, Logistic\n",
    "from blocks.initialization import IsotropicGaussian, Constant\n",
    "from theano import tensor\n",
    "from blocks.extensions.monitoring import DataStreamMonitoring\n",
    "\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.extensions import FinishAfter, Printing\n",
    "\n",
    "\n",
    "def try_model_on_mnist(model,batch_size=64,nb_epoch=4):\n",
    "    #Data Stream\n",
    "    data_stream = Flatten(DataStream.default_stream(\n",
    "        mnist,\n",
    "        iteration_scheme=SequentialScheme(mnist.num_examples, batch_size=batch_size)))\n",
    "\n",
    "    #Theano variables initialization\n",
    "\n",
    "    x = tensor.matrix('features')\n",
    "    y1 = tensor.lmatrix(name='targets')\n",
    "    y = model.apply(x)\n",
    "    model.push_initialization_config()\n",
    "    model.children[0].weights_init = Constant(0.01)\n",
    "    model.initialize()\n",
    "\n",
    "    \n",
    "    #Cost\n",
    "    cost = CategoricalCrossEntropy().apply(y1.flatten(),y)\n",
    "    algorithm = GradientDescent(cost=cost, parameters=ComputationGraph(cost).parameters,\n",
    "                                 step_rule=Scale(learning_rate=0.005))\n",
    "\n",
    "    #data stream test\n",
    "    data_stream_test = Flatten(DataStream.default_stream(\n",
    "        mnist_test,\n",
    "        iteration_scheme=SequentialScheme(\n",
    "           mnist_test.num_examples, batch_size=batch_size)))\n",
    "    \n",
    "    #Monitor\n",
    "    classification_error= MisclassificationRate().apply(np.argmax(y1),y)\n",
    "    monitor = DataStreamMonitoring(\n",
    "        variables=[cost,classification_error], data_stream=data_stream_test, prefix=\"test\")\n",
    "\n",
    "    #MainLoop\n",
    "    main_loop = MainLoop(algorithm,data_stream=data_stream, model=mlp,\n",
    "                         extensions=[monitor, FinishAfter(after_n_epochs=4), Printing()])\n",
    "    main_loop.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples from internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example to cast numpy datasets to h5py from http://fuel.readthedocs.org/en/latest/h5py_dataset.html\n",
    "import numpy\n",
    "numpy.save(\n",
    "    'train_vector_features.npy',\n",
    "    numpy.random.normal(size=(90, 10)).astype('float32'))\n",
    "numpy.save(\n",
    "    'test_vector_features.npy',\n",
    "    numpy.random.normal(size=(10, 10)).astype('float32'))\n",
    "numpy.save(\n",
    "    'train_image_features.npy',\n",
    "numpy.random.randint(2, size=(90, 3, 5, 5)).astype('uint8'))\n",
    "numpy.save(\n",
    "    'test_image_features.npy',\n",
    "    numpy.random.randint(2, size=(10, 3, 5, 5)).astype('uint8'))\n",
    "numpy.save(\n",
    "    'train_targets.npy',\n",
    "    numpy.random.randint(10, size=(90, 1)).astype('uint8'))\n",
    "numpy.save(\n",
    "    'test_targets.npy',\n",
    "    numpy.random.randint(10, size=(10, 1)).astype('uint8'))\n",
    "\n",
    "\n",
    "train_vector_features = numpy.load('train_vector_features.npy')\n",
    "test_vector_features = numpy.load('test_vector_features.npy')\n",
    "train_image_features = numpy.load('train_image_features.npy')\n",
    "test_image_features = numpy.load('test_image_features.npy')\n",
    "train_targets = numpy.load('train_targets.npy')\n",
    "test_targets = numpy.load('test_targets.npy')\n",
    "\n",
    "import h5py\n",
    "f = h5py.File('dataset.hdf5', mode='w')\n",
    "vector_features = f.create_dataset(\n",
    "    'vector_features', (100, 10), dtype='float32')\n",
    "image_features = f.create_dataset(\n",
    "    'image_features', (100, 3, 5, 5), dtype='uint8')\n",
    "targets = f.create_dataset(\n",
    "    'targets', (100, 1), dtype='uint8')\n",
    "\n",
    "vector_features[...] = numpy.vstack(\n",
    "    [train_vector_features, test_vector_features])\n",
    "image_features[...] = numpy.vstack(\n",
    "    [train_image_features, test_image_features])\n",
    "targets[...] = numpy.vstack([train_targets, test_targets])\n",
    "\n",
    "\n",
    "vector_features.dims[0].label = 'batch'\n",
    "vector_features.dims[1].label = 'feature'\n",
    "image_features.dims[0].label = 'batch'\n",
    "image_features.dims[1].label = 'channel'\n",
    "image_features.dims[2].label = 'height'\n",
    "image_features.dims[3].label = 'width'\n",
    "targets.dims[0].label = 'batch'\n",
    "targets.dims[1].label = 'index'\n",
    "\n",
    "split_array = numpy.empty(\n",
    "    6,\n",
    "    dtype=numpy.dtype([\n",
    "        ('split', 'a', 5),\n",
    "        ('source', 'a', 15),\n",
    "        ('start', numpy.int64, 1),\n",
    "        ('stop', numpy.int64, 1),\n",
    "        ('indices', h5py.special_dtype(ref=h5py.Reference)),\n",
    "        ('available', numpy.bool, 1),\n",
    "        ('comment', 'a', 1)]))\n",
    "split_array[0:3]['split'] = 'train'.encode('utf8')\n",
    "split_array[3:6]['split'] = 'test'.encode('utf8')\n",
    "split_array[0:6:3]['source'] = 'vector_features'.encode('utf8')\n",
    "split_array[1:6:3]['source'] = 'image_features'.encode('utf8')\n",
    "split_array[2:6:3]['source'] = 'targets'.encode('utf8')\n",
    "split_array[0:3]['start'] = 0\n",
    "split_array[0:3]['stop'] = 90\n",
    "split_array[3:6]['start'] = 90\n",
    "split_array[3:6]['stop'] = 100\n",
    "split_array[:]['indices'] = h5py.Reference()\n",
    "split_array[:]['available'] = True\n",
    "split_array[:]['comment'] = '.'.encode('utf8')\n",
    "f.attrs['split'] = split_array\n",
    "\n",
    "from fuel.datasets.hdf5 import H5PYDataset\n",
    "split_dict = {\n",
    "    'train': {'vector_features': (0, 90), 'image_features': (0, 90),\n",
    "              'targets': (0, 90)},\n",
    "    'test': {'vector_features': (90, 100), 'image_features': (90, 100),\n",
    "             'targets': (90, 100)}}\n",
    "f.attrs['split'] = H5PYDataset.create_split_array(split_dict)\n",
    "\n",
    "f.flush()\n",
    "f.close()\n",
    "\n",
    "#Manipulations\n",
    "\n",
    "train_set = H5PYDataset('dataset.hdf5', which_sets=('train',))\n",
    "print(train_set.num_examples)\n",
    "\n",
    "test_set = H5PYDataset('dataset.hdf5', which_sets=('test',))\n",
    "print(test_set.num_examples)\n",
    "\n",
    "train_set = H5PYDataset(\n",
    "    'dataset.hdf5', which_sets=('train',), subset=slice(0, 80))\n",
    "print(train_set.num_examples)\n",
    "valid_set = H5PYDataset(\n",
    "    'dataset.hdf5', which_sets=('train',), subset=slice(80, 90))\n",
    "print(valid_set.num_examples)\n",
    "\n",
    "print(train_set.provides_sources)\n",
    "print(train_set.axis_labels['image_features'])\n",
    "print(train_set.axis_labels['vector_features'])\n",
    "print(train_set.axis_labels['targets'])\n",
    "\n",
    "\n",
    "handle = train_set.open()\n",
    "data = train_set.get_data(handle, slice(0, 10))\n",
    "print((data[0].shape, data[1].shape, data[2].shape))\n",
    "train_set.close(handle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: An example of training a convolutional network on the MNIST dataset.\n",
      "       [-h] [--num-epochs NUM_EPOCHS]\n",
      "       [--feature-maps FEATURE_MAPS [FEATURE_MAPS ...]]\n",
      "       [--mlp-hiddens MLP_HIDDENS [MLP_HIDDENS ...]]\n",
      "       [--conv-sizes CONV_SIZES [CONV_SIZES ...]]\n",
      "       [--pool-sizes POOL_SIZES [POOL_SIZES ...]] [--batch-size BATCH_SIZE]\n",
      "       [save_to]\n",
      "An example of training a convolutional network on the MNIST dataset.: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "#lenet example\n",
    "\"\"\"Convolutional network example.\n",
    "Run the training for 50 epochs with\n",
    "```\n",
    "python __init__.py --num-epochs 50\n",
    "```\n",
    "It is going to reach around 0.8% error rate on the test set.\n",
    "\"\"\"\n",
    "import logging\n",
    "import numpy\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from theano import tensor\n",
    "\n",
    "from blocks.algorithms import GradientDescent, Scale\n",
    "from blocks.bricks import (MLP, Rectifier, Initializable, FeedforwardSequence,\n",
    "                           Softmax)\n",
    "from blocks.bricks.conv import (ConvolutionalActivation, ConvolutionalSequence,\n",
    "                                Flattener, MaxPooling)\n",
    "from blocks.bricks.cost import CategoricalCrossEntropy, MisclassificationRate\n",
    "from blocks.extensions import FinishAfter, Timing, Printing, ProgressBar\n",
    "from blocks.extensions.monitoring import (DataStreamMonitoring,\n",
    "                                          TrainingDataMonitoring)\n",
    "from blocks.extensions.saveload import Checkpoint\n",
    "from blocks.graph import ComputationGraph\n",
    "from blocks.initialization import Constant, Uniform\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.model import Model\n",
    "from blocks.monitoring import aggregation\n",
    "from fuel.datasets import MNIST\n",
    "from fuel.schemes import ShuffledScheme\n",
    "from fuel.streams import DataStream\n",
    "from toolz.itertoolz import interleave\n",
    "\n",
    "\n",
    "class LeNet(FeedforwardSequence, Initializable):\n",
    "    \"\"\"LeNet-like convolutional network.\n",
    "    The class implements LeNet, which is a convolutional sequence with\n",
    "    an MLP on top (several fully-connected layers). For details see\n",
    "    [LeCun95]_.\n",
    "    .. [LeCun95] LeCun, Yann, et al.\n",
    "       *Comparison of learning algorithms for handwritten digit\n",
    "       recognition.*,\n",
    "       International conference on artificial neural networks. Vol. 60.\n",
    "    Parameters\n",
    "    ----------\n",
    "    conv_activations : list of :class:`.Brick`\n",
    "        Activations for convolutional network.\n",
    "    num_channels : int\n",
    "        Number of channels in the input image.\n",
    "    image_shape : tuple\n",
    "        Input image shape.\n",
    "    filter_sizes : list of tuples\n",
    "        Filter sizes of :class:`.blocks.conv.ConvolutionalLayer`.\n",
    "    feature_maps : list\n",
    "        Number of filters for each of convolutions.\n",
    "    pooling_sizes : list of tuples\n",
    "        Sizes of max pooling for each convolutional layer.\n",
    "    top_mlp_activations : list of :class:`.blocks.bricks.Activation`\n",
    "        List of activations for the top MLP.\n",
    "    top_mlp_dims : list\n",
    "        Numbers of hidden units and the output dimension of the top MLP.\n",
    "    conv_step : tuples\n",
    "        Step of convolution (similar for all layers).\n",
    "    border_mode : str\n",
    "        Border mode of convolution (similar for all layers).\n",
    "    \"\"\"\n",
    "    def __init__(self, conv_activations, num_channels, image_shape,\n",
    "                 filter_sizes, feature_maps, pooling_sizes,\n",
    "                 top_mlp_activations, top_mlp_dims,\n",
    "                 conv_step=None, border_mode='valid', **kwargs):\n",
    "        if conv_step is None:\n",
    "            self.conv_step = (1, 1)\n",
    "        else:\n",
    "            self.conv_step = conv_step\n",
    "        self.num_channels = num_channels\n",
    "        self.image_shape = image_shape\n",
    "        self.top_mlp_activations = top_mlp_activations\n",
    "        self.top_mlp_dims = top_mlp_dims\n",
    "        self.border_mode = border_mode\n",
    "\n",
    "        conv_parameters = zip(conv_activations, filter_sizes, feature_maps)\n",
    "\n",
    "        # Construct convolutional layers with corresponding parameters\n",
    "        self.layers = list(interleave([\n",
    "            (ConvolutionalActivation(filter_size=filter_size,\n",
    "                                     num_filters=num_filter,\n",
    "                                     activation=activation.apply,\n",
    "                                     step=self.conv_step,\n",
    "                                     border_mode=self.border_mode,\n",
    "                                     name='conv_{}'.format(i))\n",
    "             for i, (activation, filter_size, num_filter)\n",
    "             in enumerate(conv_parameters)),\n",
    "            (MaxPooling(size, name='pool_{}'.format(i))\n",
    "             for i, size in enumerate(pooling_sizes))]))\n",
    "\n",
    "        self.conv_sequence = ConvolutionalSequence(self.layers, num_channels,\n",
    "                                                   image_size=image_shape)\n",
    "\n",
    "        # Construct a top MLP\n",
    "        self.top_mlp = MLP(top_mlp_activations, top_mlp_dims)\n",
    "\n",
    "        # We need to flatten the output of the last convolutional layer.\n",
    "        # This brick accepts a tensor of dimension (batch_size, ...) and\n",
    "        # returns a matrix (batch_size, features)\n",
    "        self.flattener = Flattener()\n",
    "        application_methods = [self.conv_sequence.apply, self.flattener.apply,\n",
    "                               self.top_mlp.apply]\n",
    "        super(LeNet, self).__init__(application_methods, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def output_dim(self):\n",
    "        return self.top_mlp_dims[-1]\n",
    "\n",
    "    @output_dim.setter\n",
    "    def output_dim(self, value):\n",
    "        self.top_mlp_dims[-1] = value\n",
    "\n",
    "    def _push_allocation_config(self):\n",
    "        self.conv_sequence._push_allocation_config()\n",
    "        conv_out_dim = self.conv_sequence.get_dim('output')\n",
    "\n",
    "        self.top_mlp.activations = self.top_mlp_activations\n",
    "        self.top_mlp.dims = [numpy.prod(conv_out_dim)] + self.top_mlp_dims\n",
    "\n",
    "\n",
    "def main(save_to, num_epochs, feature_maps=None, mlp_hiddens=None,\n",
    "         conv_sizes=None, pool_sizes=None, batch_size=500,\n",
    "         num_batches=None):\n",
    "    if feature_maps is None:\n",
    "        feature_maps = [20, 50]\n",
    "    if mlp_hiddens is None:\n",
    "        mlp_hiddens = [500]\n",
    "    if conv_sizes is None:\n",
    "        conv_sizes = [5, 5]\n",
    "    if pool_sizes is None:\n",
    "        pool_sizes = [2, 2]\n",
    "    image_size = (28, 28)\n",
    "    output_size = 10\n",
    "\n",
    "    # Use ReLUs everywhere and softmax for the final prediction\n",
    "    conv_activations = [Rectifier() for _ in feature_maps]\n",
    "    mlp_activations = [Rectifier() for _ in mlp_hiddens] + [Softmax()]\n",
    "    convnet = LeNet(conv_activations, 1, image_size,\n",
    "                    filter_sizes=zip(conv_sizes, conv_sizes),\n",
    "                    feature_maps=feature_maps,\n",
    "                    pooling_sizes=zip(pool_sizes, pool_sizes),\n",
    "                    top_mlp_activations=mlp_activations,\n",
    "                    top_mlp_dims=mlp_hiddens + [output_size],\n",
    "                    border_mode='full',\n",
    "                    weights_init=Uniform(width=.2),\n",
    "                    biases_init=Constant(0))\n",
    "    # We push initialization config to set different initialization schemes\n",
    "    # for convolutional layers.\n",
    "    convnet.push_initialization_config()\n",
    "    convnet.layers[0].weights_init = Uniform(width=.2)\n",
    "    convnet.layers[1].weights_init = Uniform(width=.09)\n",
    "    convnet.top_mlp.linear_transformations[0].weights_init = Uniform(width=.08)\n",
    "    convnet.top_mlp.linear_transformations[1].weights_init = Uniform(width=.11)\n",
    "    convnet.initialize()\n",
    "    logging.info(\"Input dim: {} {} {}\".format(\n",
    "        *convnet.children[0].get_dim('input_')))\n",
    "    for i, layer in enumerate(convnet.layers):\n",
    "        logging.info(\"Layer {} ({}) dim: {} {} {}\".format(\n",
    "            i, layer.__class__.__name__, *layer.get_dim('output')))\n",
    "\n",
    "    x = tensor.tensor4('features')\n",
    "    y = tensor.lmatrix('targets')\n",
    "\n",
    "    # Normalize input and apply the convnet\n",
    "    probs = convnet.apply(x)\n",
    "    cost = CategoricalCrossEntropy().apply(y.flatten(),\n",
    "            probs).copy(name='cost')\n",
    "    error_rate = MisclassificationRate().apply(y.flatten(), probs).copy(\n",
    "            name='error_rate')\n",
    "\n",
    "    cg = ComputationGraph([cost, error_rate])\n",
    "\n",
    "    mnist_train = MNIST((\"train\",))\n",
    "    mnist_train_stream = DataStream.default_stream(\n",
    "        mnist_train, iteration_scheme=ShuffledScheme(\n",
    "            mnist_train.num_examples, batch_size))\n",
    "\n",
    "    mnist_test = MNIST((\"test\",))\n",
    "    mnist_test_stream = DataStream.default_stream(\n",
    "        mnist_test,\n",
    "        iteration_scheme=ShuffledScheme(\n",
    "            mnist_test.num_examples, batch_size))\n",
    "\n",
    "    # Train with simple SGD\n",
    "    algorithm = GradientDescent(\n",
    "        cost=cost, parameters=cg.parameters,\n",
    "        step_rule=Scale(learning_rate=0.1))\n",
    "    # `Timing` extension reports time for reading data, aggregating a batch\n",
    "    # and monitoring;\n",
    "    # `ProgressBar` displays a nice progress bar during training.\n",
    "    extensions = [Timing(),\n",
    "                  FinishAfter(after_n_epochs=num_epochs,\n",
    "                              after_n_batches=num_batches),\n",
    "                  DataStreamMonitoring(\n",
    "                      [cost, error_rate],\n",
    "                      mnist_test_stream,\n",
    "                      prefix=\"test\"),\n",
    "                  TrainingDataMonitoring(\n",
    "                      [cost, error_rate,\n",
    "                       aggregation.mean(algorithm.total_gradient_norm)],\n",
    "                      prefix=\"train\",\n",
    "                      after_epoch=True),\n",
    "                  Checkpoint(save_to),\n",
    "                  ProgressBar(),\n",
    "                  Printing()]\n",
    "\n",
    "    model = Model(cost)\n",
    "\n",
    "    main_loop = MainLoop(\n",
    "        algorithm,\n",
    "        mnist_train_stream,\n",
    "        model=model,\n",
    "        extensions=extensions)\n",
    "\n",
    "    main_loop.run()\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    parser = ArgumentParser(\"An example of training a convolutional network \"\n",
    "                            \"on the MNIST dataset.\")\n",
    "    parser.add_argument(\"--num-epochs\", type=int, default=2,\n",
    "                        help=\"Number of training epochs to do.\")\n",
    "    parser.add_argument(\"save_to\", default=\"mnist.pkl\", nargs=\"?\",\n",
    "                        help=\"Destination to save the state of the training \"\n",
    "                             \"process.\")\n",
    "    parser.add_argument(\"--feature-maps\", type=int, nargs='+',\n",
    "                        default=[20, 50], help=\"List of feature maps numbers.\")\n",
    "    parser.add_argument(\"--mlp-hiddens\", type=int, nargs='+', default=[500],\n",
    "                        help=\"List of numbers of hidden units for the MLP.\")\n",
    "    parser.add_argument(\"--conv-sizes\", type=int, nargs='+', default=[5, 5],\n",
    "                        help=\"Convolutional kernels sizes. The kernels are \"\n",
    "                        \"always square.\")\n",
    "    parser.add_argument(\"--pool-sizes\", type=int, nargs='+', default=[2, 2],\n",
    "                        help=\"Pooling sizes. The pooling windows are always \"\n",
    "                             \"square. Should be the same length as \"\n",
    "                             \"--conv-sizes.\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=500,\n",
    "                        help=\"Batch size.\")\n",
    "    args = parser.parse_args()\n",
    "    main(**vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.  1.  1.]]\n",
      "\n",
      " [[ 2.  2.  2.]]\n",
      "\n",
      " [[ 3.  3.  3.]]]\n"
     ]
    }
   ],
   "source": [
    "#RNN example from : http://blocks.readthedocs.org/en/latest/rnn.html\n",
    "\n",
    "import numpy\n",
    "import theano\n",
    "from theano import tensor\n",
    "from blocks import initialization\n",
    "from blocks.bricks import Identity\n",
    "from blocks.bricks.recurrent import SimpleRecurrent\n",
    "x = tensor.tensor3('x')\n",
    "rnn = SimpleRecurrent(\n",
    "    dim=3, activation=Identity(), weights_init=initialization.Identity())\n",
    "rnn.initialize()\n",
    "h = rnn.apply(x)\n",
    "f = theano.function([x], h)\n",
    "print(f(numpy.ones((3, 1, 3), dtype=theano.config.floatX))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from blocks.algorithms.StepRule import StepRule\n",
    "#http://blocks.readthedocs.org/en/latest/api/algorithms.html?highlight=scale#blocks.algorithms.StepRule\n",
    "class MyOwnStepRule(StepRule)\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def compute_step(parameter,previous_step):\n",
    "        pass\n",
    "    \n",
    "    def compute_steps(previous_steps):\n",
    "        pass\n",
    "\n",
    "from blocks.bricks.conv import Convolutional\n",
    "\n",
    "class HomeMadeConvNet(Feedfoward, Initializable):\n",
    "    def __init__(self,filter_size=(5,5),num_channels=1):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "\n",
    "class HomeMadeMaxoutNet(FeedforwardSequence, Initializable):\n",
    "    def __init__(self, activations_layers):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

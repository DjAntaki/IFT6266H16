{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://en.wikipedia.org/wiki/Image_scaling#Algorithms\n",
    "\n",
    "to do :\n",
    "- own validation set\n",
    "- resize images transformer (Bilinear done?)\n",
    "- make noise transformer\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#I don't think its working yet.\n",
    "from __future__ import division\n",
    "import math\n",
    "from fractions import Fraction\n",
    "\n",
    "from fuel.transformers import ExpectsAxisLabels, SourcewiseTransformer\n",
    "\n",
    "class BilinearRescale(SourcewiseTransformer, ExpectsAxisLabels):\n",
    "    \"\"\"Resize an image to a fixed window size. Use bilinear interpolation with 4-relative nearest neighbors.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_stream : :class:`AbstractDataStream`\n",
    "        The data stream to wrap.\n",
    "    window_shape : tuple\n",
    "        The `(height, width)` tuple representing the size of the output\n",
    "        window.\n",
    "    Notes\n",
    "    -----\n",
    "    This transformer expects to act on stream sources which provide one of\n",
    "     * Single images represented as 3-dimensional ndarrays, with layout\n",
    "       `(channel, height, width)`.\n",
    "     * Batches of images represented as lists of 3-dimensional ndarrays,\n",
    "       possibly of different shapes (i.e. images of differing\n",
    "       heights/widths).\n",
    "     * Batches of images represented as 4-dimensional ndarrays, with\n",
    "       layout `(batch, channel, height, width)`.\n",
    "    The format of the stream will be un-altered, i.e. if lists are\n",
    "    yielded by `data_stream` then lists will be yielded by this\n",
    "    transformer.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_stream, image_shape, **kwargs):\n",
    "        self.image_shape = image_shape\n",
    "        kwargs.setdefault('produces_examples', data_stream.produces_examples)\n",
    "        kwargs.setdefault('axis_labels', data_stream.axis_labels)\n",
    "        super(BilinearRescale, self).__init__(data_stream, **kwargs)\n",
    "\n",
    "    def transform_source_batch(self, source, source_name):\n",
    "        self.verify_axis_labels(('batch', 'channel', 'height', 'width'),\n",
    "                                self.data_stream.axis_labels[source_name],\n",
    "                                source_name)\n",
    "        height, width = self.image_shape\n",
    "        print(source.shape)\n",
    "        if isinstance(source, np.ndarray) and source.ndim == 4:\n",
    "            # Not yet supported(batch, channels, height, width).\n",
    "            raise Exception\n",
    "        \n",
    "        elif all(isinstance(b, np.ndarray) and b.ndim == 3 for b in source):\n",
    "            return [self.transform_source_example(im, source_name)\n",
    "                    for im in source]\n",
    "        else:\n",
    "            raise ValueError(\"uninterpretable batch format; expected a list \"\n",
    "                             \"of arrays with ndim = 3, or an array with \"\n",
    "                             \"ndim = 4\")\n",
    "\n",
    "    def transform_source_example(self, example, source_name):\n",
    "        self.verify_axis_labels(('channel', 'height', 'width'),\n",
    "                                self.data_stream.axis_labels[source_name],\n",
    "                                source_name)\n",
    "        height, width = self.image_shape\n",
    "        nb_channel = example.shape[0] #That line could be replace by something more elegant.\n",
    "        \n",
    "        if not isinstance(example, np.ndarray) or example.ndim != 3:\n",
    "            raise ValueError(\"uninterpretable example format; expected \"\n",
    "                             \"ndarray with ndim = 3\")\n",
    "        image_height, image_width = example.shape[1:]\n",
    "        #rescale_height, rescale_width = (image_height-1)/(height-1), (image_width-1)/(width-1)\n",
    "        rescale_height, rescale_width = Fraction(image_height-1,height-1), Fraction(image_width-1,width-1)\n",
    "        \n",
    "        rescaled_image = np.zeros((nb_channel,height,width))\n",
    "        #That code ain't pretty. Might do a cleaner version eventually\n",
    "        \n",
    "        for i,j in product(range(height), range(width)):\n",
    "            x, y  = i*rescale_height, j * rescale_width\n",
    "            x1 = np.array([math.floor(x), math.ceil(x)],dtype=np.intp)\n",
    "            y1 = np.array([math.floor(y), math.ceil(y)],dtype=np.intp)\n",
    "            dx,dy = x - x1[0], y - y1[0]\n",
    "            del_x, del_y = x1[1]-x1[0], y1[1]-y1[0]\n",
    "            \n",
    "            #Stupid patching for float approximation induced problem\n",
    "            if x1[1] == image_height:\n",
    "                x1[1] -= 1\n",
    "            if y1[1] == image_width:\n",
    "                y1[1] -= 1\n",
    "                \n",
    "            if not x1[0] == x1[1] and not y1[0] == y1[1] :\n",
    "                for c in range(nb_channel):\n",
    "                    xy1,xy2 = example[c][np.ix_(x1,y1)]\n",
    "                    x1y1,x2y1,x1y2,x2y2 = xy1[0],xy1[1],xy2[0],xy2[1]\n",
    "                    rescaled_image[c,i,j] = (x2y1-x1y1)*dx/del_x + (x1y2 -x1y1) *dy/del_y + (x1y1 + x2y2 - x2y1 - x1y2) *dx/del_x*dy/del_y + x1y1\n",
    "            elif x1[0] == x1[1] and y1[0] == y1[1]:\n",
    "                rescaled_image[:,i,j] = example[:,x1[0],y1[0]]\n",
    "            else:\n",
    "                if y1[0] == y1[1]:\n",
    "                    for c in range(nb_channel):\n",
    "                        x1y,x2y = example[c][np.ix_(x1,y1)]\n",
    "                        x1y,x2y = x1y[0], x1y[1]\n",
    "                        rescaled_image[c,i,j] = (x2y-x1y)*dx/del_x + x1y\n",
    "                else:\n",
    "                    for c in range(nb_channel):\n",
    "                        xy1,xy2 = example[c][np.ix_(x1,y1)]\n",
    "                        xy1,xy2 = xy1[0], xy1[1]\n",
    "                        rescaled_image[c,i,j] = (xy2 - xy1) * dy/del_y + xy1\n",
    "            \n",
    "        return rescaled_image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This section deals with the dogs_vs_cats problem with a simple MLP. It's mainly for testing the BilinearRescale\n",
    "\n",
    "s0,s1 = 100,100\n",
    "nb_hidden = 600\n",
    "num_epochs = 1\n",
    "batch_size = 1\n",
    "num_batches = 25\n",
    "\n",
    "# Let's load and process the dataset\n",
    "import numpy as np\n",
    "from fuel.datasets.dogs_vs_cats import DogsVsCats\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import ShuffledScheme\n",
    "from fuel.transformers.image import RandomFixedSizeCrop\n",
    "from fuel.transformers import Flatten\n",
    "\n",
    "train = DogsVsCats(('train',), subset=slice(0, 20000))\n",
    "test = DogsVsCats(('test',), subset=slice(0,100))\n",
    "\n",
    "train_stream = DataStream.default_stream(\n",
    "    train,\n",
    "    iteration_scheme=ShuffledScheme(train.num_examples, batch_size)\n",
    ")\n",
    "\n",
    "test_stream = DataStream.default_stream(\n",
    "    test,\n",
    "    iteration_scheme=ShuffledScheme(test.num_examples, batch_size)\n",
    ")\n",
    "\n",
    "train_stream = BilinearRescale(train_stream, (s0,s1), which_sources=('image_features',))\n",
    "test_stream = BilinearRescale(test_stream, (s0,s1), which_sources=('image_features',))\n",
    "\n",
    "if True:\n",
    "    train_stream = Flatten(train_stream, which_sources=('image_features',))\n",
    "    test_stream = Flatten(test_stream, which_sources=('image_features',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------\n",
      "BEFORE FIRST EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 0\n",
      "\t received_first_batch: False\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 0:\n",
      "\t time_initialization: 0.199960947037\n",
      "\n",
      "(1,)\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 1\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 1:\n",
      "\n",
      "(1,)\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 2\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 2:\n",
      "\n",
      "(1,)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:blocks.main_loop:Received epoch interrupt signal.\n",
      "\n",
      "Blocks will complete this epoch of training and run extensions before exiting. If you do not want to complete this epoch, press CTRL + C again to stop training after the current batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: True\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 3\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 3:\n",
      "\n",
      "(1,)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:blocks.main_loop:Received batch interrupt signal.\n",
      "\n",
      "Blocks will complete the current batch and run extensions before exiting. If you do not want to complete this batch, press CTRL + C again. WARNING: Note that this will end training immediately, and extensions that e.g. save your training progress won't be run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: True\n",
      "\t epoch_interrupt_received: True\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 4\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 4:\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "TRAINING HAS BEEN FINISHED:\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: True\n",
      "\t epoch_interrupt_received: True\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 4\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 4:\n",
      "\t training_finished: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "from itertools import product\n",
    "from theano import tensor\n",
    "    \n",
    "X = tensor.matrix('image_features')\n",
    "T = tensor.lmatrix('targets')\n",
    "W = theano.shared(\n",
    "    np.random.uniform(low=-0.01, high=0.01, size=(s0*s1*3, nb_hidden)), 'W')\n",
    "b = theano.shared(np.zeros(nb_hidden))\n",
    "V = theano.shared(\n",
    "    np.random.uniform(low=-0.01, high=0.01, size=(nb_hidden, 2)), 'V')\n",
    "c = theano.shared(np.zeros(2))\n",
    "params = [W, b, V, c]\n",
    "\n",
    "H = tensor.nnet.sigmoid(tensor.dot(X, W) + b)\n",
    "Y = tensor.nnet.softmax(tensor.dot(H, V) + c)\n",
    "\n",
    "# Use Blocks to train this network\n",
    "from blocks.algorithms import GradientDescent, Scale\n",
    "from blocks.extensions import Printing\n",
    "from blocks.extensions import FinishAfter, Timing, Printing, ProgressBar\n",
    "from blocks.extensions.monitoring import (DataStreamMonitoring,\n",
    "                                          TrainingDataMonitoring)\n",
    "from blocks.bricks.cost import CategoricalCrossEntropy,MisclassificationRate\n",
    "from blocks.monitoring import aggregation\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.model import Model \n",
    "\n",
    "loss = tensor.nnet.categorical_crossentropy(Y, T.flatten()).mean()\n",
    "\n",
    "#classification_error= MisclassificationRate().apply(tensor.argmax(Y),T)\n",
    "classification_error= MisclassificationRate().apply(T.flatten(),Y)\n",
    "#squared_error = SquaredError().apply(Y.T,tensor.eye(10)[T])\n",
    "\n",
    "algorithm = GradientDescent(cost=loss, parameters=params,\n",
    "                            step_rule=Scale(learning_rate=0.1))\n",
    "\n",
    "# We want to monitor the cost as we train\n",
    "loss.name = 'loss'\n",
    "extensions = [Timing(), FinishAfter(after_n_epochs=num_epochs,\n",
    "                              after_n_batches=num_batches),\n",
    "              TrainingDataMonitoring([loss,classification_error],prefix=\"train\",after_epoch=True),    \n",
    "            #  DataStreamMonitoring(\n",
    "           #           [loss],#classification_error\n",
    "          #            test_stream,prefix=\"test\"),\n",
    "                  #ProgressBar(),\n",
    "                 Printing(every_n_batches=1)]\n",
    "\n",
    "#extensions = [Timing(),\n",
    "#              FinishAfter(after_n_epochs=num_epochs),\n",
    "#              TrainingDataMonitoring([loss,classification_error],prefix=\"train\",after_epoch=True),\n",
    "             #   DataStreamMonitoring([loss, classification_error],test_stream,prefix=\"test\"),\n",
    "             #    ProgressBar(),\n",
    "#                  Printing() ]\n",
    "\n",
    "#TrainingDataMonitoring([loss, classification_error,squared_error,aggregation.mean(algorithm.total_gradient_norm)], \n",
    "      #                  every_n_batches=5,\n",
    "       #               prefix=\"train\",\n",
    "        #              after_epoch=True)\n",
    "#model = Model(loss)\n",
    "\n",
    "main_loop = MainLoop(data_stream=train_stream, algorithm=algorithm,\n",
    "                     extensions=extensions)\n",
    "main_loop.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "I was going try coding a CNN, but then i figured that i could probably reuse the LeNet example in blocks-example\n",
    "\n",
    "Link to the depot : https://github.com/mila-udem/blocks-examples/blob/master/mnist_lenet/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from theano import tensor\n",
    "\n",
    "from blocks.algorithms import GradientDescent, Scale\n",
    "from blocks.bricks import (Rectifier, Initializable,\n",
    "                           Softmax)\n",
    "from blocks.bricks.cost import CategoricalCrossEntropy, MisclassificationRate\n",
    "from blocks.extensions import FinishAfter, Timing, Printing, ProgressBar\n",
    "from blocks.extensions.monitoring import (DataStreamMonitoring,\n",
    "                                          TrainingDataMonitoring)\n",
    "from blocks.extensions.saveload import Checkpoint\n",
    "from blocks.graph import ComputationGraph\n",
    "from blocks.initialization import Constant, Uniform\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.model import Model\n",
    "from blocks.monitoring import aggregation\n",
    "from fuel.schemes import ShuffledScheme\n",
    "from fuel.streams import DataStream\n",
    "from toolz.itertoolz import interleave\n",
    "from itertools import product\n",
    "    \n",
    "from LeNet import LeNet\n",
    "\n",
    "def main_convnet(save_to, num_epochs, train, test, input_size=None, feature_maps=None, mlp_hiddens=None,\n",
    "         conv_sizes=None, pool_sizes=None, batch_size=50, num_batches=None):\n",
    "    if feature_maps is None:\n",
    "        feature_maps = [20, 50]\n",
    "    if mlp_hiddens is None:\n",
    "        mlp_hiddens = [500]\n",
    "    if conv_sizes is None:\n",
    "        conv_sizes = [5, 5]\n",
    "    if pool_sizes is None:\n",
    "        pool_sizes = [2, 2]\n",
    "    if input_size is None :\n",
    "        input_size = (150, 150)\n",
    "    output_size = 2\n",
    "    \n",
    "    # Use ReLUs everywhere and softmax for the final prediction\n",
    "    conv_activations = [Rectifier() for _ in feature_maps]\n",
    "    mlp_activations = [Rectifier() for _ in mlp_hiddens] + [Softmax()]\n",
    "    convnet = LeNet(conv_activations, 3, input_size,\n",
    "                    filter_sizes=zip(conv_sizes, conv_sizes),\n",
    "                    feature_maps=feature_maps,\n",
    "                    pooling_sizes=zip(pool_sizes, pool_sizes),\n",
    "                    top_mlp_activations=mlp_activations,\n",
    "                    top_mlp_dims=mlp_hiddens + [output_size],\n",
    "                    border_mode='full',\n",
    "                    weights_init=Uniform(width=.2),\n",
    "                    biases_init=Constant(0))\n",
    "    \n",
    "    \n",
    "    # We push initialization config to set different initialization schemes\n",
    "    # for convolutional layers.\n",
    "    convnet.push_initialization_config()\n",
    "    convnet.layers[0].weights_init = Uniform(width=.2)\n",
    "    convnet.layers[1].weights_init = Uniform(width=.09)\n",
    "    convnet.top_mlp.linear_transformations[0].weights_init = Uniform(width=.08)\n",
    "    convnet.top_mlp.linear_transformations[1].weights_init = Uniform(width=.11)\n",
    "    convnet.initialize()\n",
    "    logging.info(\"Input dim: {} {} {}\".format(\n",
    "        *convnet.children[0].get_dim('input_')))\n",
    "    for i, layer in enumerate(convnet.layers):\n",
    "        logging.info(\"Layer {} ({}) dim: {} {} {}\".format(\n",
    "            i, layer.__class__.__name__, *layer.get_dim('output')))\n",
    "\n",
    "    x = tensor.tensor4('image_features')\n",
    "    y = tensor.lmatrix('targets')\n",
    "\n",
    "    # Normalize input and apply the convnet\n",
    "    probs = convnet.apply(x)\n",
    "    cost = CategoricalCrossEntropy().apply(y.flatten(),\n",
    "            probs).copy(name='cost')\n",
    "    error_rate = MisclassificationRate().apply(y.flatten(), probs).copy(\n",
    "            name='error_rate')\n",
    "\n",
    "    cg = ComputationGraph([cost, error_rate]) #2 cost?\n",
    "\n",
    "    \n",
    "    #Generating stream\n",
    "    train_stream = DataStream.default_stream(\n",
    "        train,\n",
    "        iteration_scheme=ShuffledScheme(train.num_examples, batch_size)\n",
    "    )\n",
    "\n",
    "    test_stream = DataStream.default_stream(\n",
    "        test,\n",
    "        iteration_scheme=ShuffledScheme(test.num_examples, batch_size)\n",
    "    )\n",
    "    \n",
    "    train_stream = BilinearRescale(train_stream, input_size, which_sources=('image_features',))\n",
    "    test_stream = BilinearRescale(test_stream, input_size, which_sources=('image_features',))\n",
    "\n",
    "    # Train with simple SGD\n",
    "    algorithm = GradientDescent(\n",
    "        cost=cost, parameters=cg.parameters,\n",
    "        step_rule=Scale(learning_rate=0.1))\n",
    "    # `Timing` extension reports time for reading data, aggregating a batch\n",
    "    # and monitoring;\n",
    "    # `ProgressBar` displays a nice progress bar during training.\n",
    "    extensions = [Timing(),\n",
    "                  FinishAfter(after_n_epochs=num_epochs,\n",
    "                              after_n_batches=num_batches),\n",
    "                  DataStreamMonitoring(\n",
    "                      [cost, error_rate],\n",
    "                      test_stream,\n",
    "                      prefix=\"test\"),\n",
    "                  TrainingDataMonitoring(\n",
    "                      [cost, error_rate,\n",
    "                       aggregation.mean(algorithm.total_gradient_norm)],\n",
    "                      prefix=\"train\",\n",
    "                      after_epoch=True),\n",
    "                  Checkpoint(save_to),\n",
    "                  ProgressBar(),\n",
    "                  Printing(every_n_batches=1)]\n",
    "\n",
    "    model = Model(cost)\n",
    "\n",
    "    main_loop = MainLoop(\n",
    "        algorithm,\n",
    "        train_stream,\n",
    "        model=model,\n",
    "        extensions=extensions)\n",
    "\n",
    "    main_loop.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's load and process the dataset\n",
    "import numpy as np\n",
    "from fuel.datasets.dogs_vs_cats import DogsVsCats\n",
    "\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import ShuffledScheme\n",
    "from fuel.transformers.image import RandomFixedSizeCrop\n",
    "from fuel.transformers import Flatten\n",
    "\n",
    "# Load the training set\n",
    "train = DogsVsCats(('train',), subset=slice(0, 200))\n",
    "test = DogsVsCats(('test',), subset=slice(0,2000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:blocks.main_loop:Received epoch interrupt signal.\n",
      "\n",
      "Blocks will complete this epoch of training and run extensions before exiting. If you do not want to complete this epoch, press CTRL + C again to stop training after the current batch.\n",
      "WARNING:blocks.main_loop:Received batch interrupt signal.\n",
      "\n",
      "Blocks will complete the current batch and run extensions before exiting. If you do not want to complete this batch, press CTRL + C again. WARNING: Note that this will end training immediately, and extensions that e.g. save your training progress won't be run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ntak/.local/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 970, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/ntak/.local/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 233, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ntak/.local/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 267, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 1044, in getinnerframes\n",
      "    framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 1008, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"/home/ntak/.local/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 171, in findsource\n",
      "    lines = linecache.getlines(file, globals_dict)\n",
      "  File \"/usr/lib/python2.7/linecache.py\", line 41, in getlines\n",
      "    return updatecache(filename, module_globals)\n",
      "  File \"/usr/lib/python2.7/linecache.py\", line 132, in updatecache\n",
      "    lines = fp.readlines()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "Unfortunately, your original traceback can not be constructed.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ntak/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_code\u001b[1;34m(self, code_obj, result)\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3082\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3083\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3084\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3085\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ntak/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only)\u001b[0m\n\u001b[0;32m   1878\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1879\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[1;32m-> 1880\u001b[1;33m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[0;32m   1881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ntak/.local/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1240\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[1;32m-> 1242\u001b[1;33m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[0;32m   1243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ntak/.local/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1148\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[1;32m-> 1150\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1151\u001b[0m             )\n\u001b[0;32m   1152\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ntak/.local/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[1;32m-> 1002\u001b[1;33m                                                                tb_offset)\n\u001b[0m\u001b[0;32m   1003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[1;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ntak/.local/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[0mrecords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrecords\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ntak/.local/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mformat_records\u001b[1;34m(self, records)\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m         \u001b[0mabspath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 724\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    725\u001b[0m             \u001b[1;31m#print '*** record:',file,lnum,func,lines,index  # dbg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "main_convnet(\"lenetsave.txt\", 1, train, test, batch_size=1, num_batches=10)\n",
    "\n",
    "#main(save_to, num_epochs, train, test, input_size=None, feature_maps=None, mlp_hiddens=None,\n",
    "#         conv_sizes=None, pool_sizes=None, batch_size=500,flatten_stream=False,\n",
    "#         num_batches=None):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%debug #Fucking hell ce bug. Jy comprends rien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:blocks.main_loop:Received epoch interrupt signal.\n",
      "\n",
      "Blocks will complete this epoch of training and run extensions before exiting. If you do not want to complete this epoch, press CTRL + C again to stop training after the current batch.\n",
      "WARNING:blocks.main_loop:Received batch interrupt signal.\n",
      "\n",
      "Blocks will complete the current batch and run extensions before exiting. If you do not want to complete this batch, press CTRL + C again. WARNING: Note that this will end training immediately, and extensions that e.g. save your training progress won't be run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-0d661f7bac0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     92\u001b[0m           weights_init=IsotropicGaussian(), biases_init=Constant(0.01))\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mlp1.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflatten_stream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-0d661f7bac0b>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(save_to, model, num_epochs, train, test, input_size, batch_size, learning_rate, flatten_stream, num_batches)\u001b[0m\n\u001b[0;32m     79\u001b[0m     main_loop = MainLoop(data_stream=train_stream, algorithm=algorithm,\n\u001b[0;32m     80\u001b[0m                          extensions=extensions)\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[0mmain_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ntak/.local/lib/python2.7/site-packages/blocks/main_loop.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    181\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batch_interrupt_received'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mTimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m                     \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m                         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTrainingFinish\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ntak/.local/lib/python2.7/site-packages/blocks/main_loop.pyc\u001b[0m in \u001b[0;36m_run_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    228\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epoch_started'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_extensions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'before_epoch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mTimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ntak/.local/lib/python2.7/site-packages/blocks/main_loop.pyc\u001b[0m in \u001b[0;36m_run_extensions\u001b[1;34m(self, method_name, *args)\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mextension\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextensions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mTimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextension\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m                     \u001b[0mextension\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCallbackName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_finish_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ntak/.local/lib/python2.7/site-packages/blocks/extensions/__init__.pyc\u001b[0m in \u001b[0;36mdispatch\u001b[1;34m(self, callback_invoked, *from_main_loop)\u001b[0m\n\u001b[0;32m    336\u001b[0m             if (callback_name == callback_invoked and\n\u001b[0;32m    337\u001b[0m                     predicate(self.main_loop.log)):\n\u001b[1;32m--> 338\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback_invoked\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrom_main_loop\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ntak/.local/lib/python2.7/site-packages/blocks/extensions/monitoring.pyc\u001b[0m in \u001b[0;36mdo\u001b[1;34m(self, callback_name, *args)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;34m\"\"\"Write the values of monitored variables to the log.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Monitoring on auxiliary data started\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mvalue_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_stream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Monitoring on auxiliary data finished\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ntak/.local/lib/python2.7/site-packages/blocks/monitoring/evaluators.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, data_stream)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_aggregators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accumulate_fun\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_epoch_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mas_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ntak/.local/lib/python2.7/site-packages/six.pyc\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m     \u001b[0mcallable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ntak/.local/lib/python2.7/site-packages/fuel/iterator.pyc\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msources\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ntak/.local/lib/python2.7/site-packages/fuel/transformers/__init__.pyc\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrequest\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchild_epoch_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproduces_examples\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproduces_examples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ntak/.local/lib/python2.7/site-packages/six.pyc\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m     \u001b[0mcallable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ntak/.local/lib/python2.7/site-packages/fuel/iterator.pyc\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msources\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ntak/.local/lib/python2.7/site-packages/fuel/transformers/__init__.pyc\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    149\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform_example\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtransform_example\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ntak/.local/lib/python2.7/site-packages/fuel/transformers/__init__.pyc\u001b[0m in \u001b[0;36mtransform_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtransform_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         return self._apply_sourcewise_transformation(\n\u001b[1;32m--> 289\u001b[1;33m             data=batch, method=self.transform_source_batch)\n\u001b[0m\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ntak/.local/lib/python2.7/site-packages/fuel/transformers/__init__.pyc\u001b[0m in \u001b[0;36m_apply_sourcewise_transformation\u001b[1;34m(self, data, method)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msources\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msource_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhich_sources\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m                 \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-03ac7fb42a39>\u001b[0m in \u001b[0;36mtransform_source_batch\u001b[1;34m(self, source, source_name)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             return [self.transform_source_example(im, source_name)\n\u001b[1;32m---> 49\u001b[1;33m                     for im in source]\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             raise ValueError(\"uninterpretable batch format; expected a list \"\n",
      "\u001b[1;32m<ipython-input-1-03ac7fb42a39>\u001b[0m in \u001b[0;36mtransform_source_example\u001b[1;34m(self, example, source_name)\u001b[0m\n\u001b[0;32m     87\u001b[0m                     \u001b[0mxy1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mxy2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                     \u001b[0mx1y1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx2y1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx1y2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx2y2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxy1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mxy1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mxy2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mxy2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                     \u001b[0mrescaled_image\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx2y1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx1y1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdx\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdel_x\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx1y2\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mx1y1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mdy\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdel_y\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx1y1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx2y2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mx2y1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mx1y2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mdx\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdel_x\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdy\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdel_y\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx1y1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mrescaled_image\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/fractions.pyc\u001b[0m in \u001b[0;36mreverse\u001b[1;34m(b, a)\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[0mforward\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmonomorphic_operator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 372\u001b[1;33m         \u001b[1;32mdef\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    373\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRational\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m                 \u001b[1;31m# Includes ints.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import theano\n",
    "from itertools import product\n",
    "from blocks.graph import ComputationGraph\n",
    "# Use Blocks to train this network\n",
    "from blocks.algorithms import GradientDescent, Scale\n",
    "from blocks.extensions import Printing\n",
    "from blocks.extensions import FinishAfter, Timing, Printing, ProgressBar\n",
    "from blocks.extensions.monitoring import (DataStreamMonitoring,\n",
    "                                          TrainingDataMonitoring)\n",
    "from blocks.bricks.cost import CategoricalCrossEntropy,MisclassificationRate\n",
    "from blocks.monitoring import aggregation\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.model import Model \n",
    "\n",
    "def main(save_to, model, num_epochs, train, test, input_size = (150,150), batch_size=50, learning_rate=0.05,flatten_stream=False,\n",
    "         num_batches=10):\n",
    "\n",
    "    #Variables theano\n",
    "    x = tensor.matrix('image_features')\n",
    "    T = tensor.lmatrix(name='targets')\n",
    "    Y = mlp.apply(x)\n",
    "\n",
    "    model.push_initialization_config()\n",
    "    #model.children[0].weights_init = Constant(0.01)\n",
    "    model.initialize()\n",
    "\n",
    "    #Cost\n",
    "#    loss = tensor.nnet.categorical_crossentropy(Y, T.flatten()).mean()\n",
    "    #classification_error= MisclassificationRate().apply(tensor.argmax(Y),T)\n",
    "    classification_error= MisclassificationRate().apply(T.flatten(),Y)\n",
    "    #squared_error = SquaredError().apply(Y.T,tensor.eye(10)[T])\n",
    "    cost = CategoricalCrossEntropy().apply(T.flatten(),Y)\n",
    "    cg = ComputationGraph(cost)\n",
    "    algorithm = GradientDescent(cost=cost, parameters=cg.parameters,\n",
    "                             step_rule=Scale(learning_rate=learning_rate))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    #Generating stream\n",
    "    train_stream = DataStream.default_stream(\n",
    "        train,\n",
    "        iteration_scheme=ShuffledScheme(train.num_examples, batch_size)\n",
    "    )\n",
    "\n",
    "    test_stream = DataStream.default_stream(\n",
    "        test,\n",
    "        iteration_scheme=ShuffledScheme(test.num_examples, batch_size)\n",
    "    )\n",
    "\n",
    "\n",
    "    #Rescale images\n",
    "    train_stream = BilinearRescale(train_stream, input_size, which_sources=('image_features',))\n",
    "    test_stream = BilinearRescale(test_stream, input_size, which_sources=('image_features',)) \n",
    "    \n",
    "    #Flattening the stream\n",
    "    if flatten_stream is True:\n",
    "        train_stream = Flatten(train_stream, which_sources=('image_features',))\n",
    "        test_stream = Flatten(test_stream, which_sources=('image_features',))\n",
    "\n",
    "        \n",
    "    #Monitoring\n",
    "    cost.name = 'cost'\n",
    "    extensions = [Timing(), FinishAfter(after_n_epochs=num_epochs,\n",
    "                                  after_n_batches=num_batches),\n",
    "                        TrainingDataMonitoring([cost,classification_error],prefix=\"train\"),    \n",
    "\n",
    "                      DataStreamMonitoring([cost, classification_error], test_stream,prefix=\"test\"),\n",
    "                      #ProgressBar(),\n",
    "                     Printing(every_n_batches=1)]\n",
    "\n",
    "    #TrainingDataMonitoring([loss, classification_error,squared_error,aggregation.mean(algorithm.total_gradient_norm)], \n",
    "          #                  every_n_batches=5,\n",
    "           #               prefix=\"train\",\n",
    "            #              after_epoch=True)\n",
    "    #model = Model(loss)\n",
    "\n",
    "    main_loop = MainLoop(data_stream=train_stream, algorithm=algorithm,\n",
    "                         extensions=extensions)\n",
    "    main_loop.run()\n",
    "\n",
    "\n",
    "#Model\n",
    "from blocks.bricks import Tanh,Linear, Rectifier, Softmax, MLP, Logistic\n",
    "from blocks.initialization import IsotropicGaussian, Constant\n",
    "from theano import tensor\n",
    "\n",
    "input_size = (150,150)\n",
    "\n",
    "mlp = MLP(activations=[Rectifier(name='rect0'),Logistic(name='sigmoid_1'),Softmax(name='softmax_2')], dims=[input_size[0]*input_size[1]*3, 1000, 500, 2],\n",
    "          weights_init=IsotropicGaussian(), biases_init=Constant(0.01))\n",
    "\n",
    "main('mlp1.txt',mlp,2,train,test,input_size = input_size, batch_size=1, flatten_stream=True, num_batches=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cuda lib_dirs error (it isnt in lib_dirs) cuda shared libra\n",
    "LD_LIBRAIRY_\n",
    "- Wierd import error with LeNet.\n",
    "- retrieve information from test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

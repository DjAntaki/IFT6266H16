{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://en.wikipedia.org/wiki/Image_scaling#Algorithms\n",
    "\n",
    "to do :\n",
    "- own validation set\n",
    "- resize images transformer (Bilinear done!)\n",
    "- make noise transformer\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's load and process the dataset\n",
    "import numpy as np\n",
    "from fuel.datasets.dogs_vs_cats import DogsVsCats\n",
    "train = DogsVsCats(('train',), subset=slice(0, 20000))\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import ShuffledScheme\n",
    "from fuel.transformers.image import RandomFixedSizeCrop\n",
    "from fuel.transformers import Flatten\n",
    "\n",
    "# Load the training set\n",
    "train = DogsVsCats(('train',), subset=slice(0, 100))\n",
    "\n",
    "# We now create a \"stream\" over the dataset which will return shuffled batches\n",
    "# of size 128. Using the DataStream.default_stream constructor will turn our\n",
    "# 8-bit images into floating-point decimals in [0, 1].\n",
    "stream = DataStream.default_stream(\n",
    "    train,\n",
    "    iteration_scheme=ShuffledScheme(train.num_examples, 128)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This whole cell is copy-paste from https://ift6266h16.wordpress.com/class-project/getting-started/\n",
    "\n",
    "\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import ShuffledScheme\n",
    "from fuel.transformers.image import RandomFixedSizeCrop\n",
    "from fuel.transformers import Flatten\n",
    "\n",
    "# Load the training set\n",
    "train = DogsVsCats(('train',), subset=slice(0, 20000))\n",
    "\n",
    "# We now create a \"stream\" over the dataset which will return shuffled batches\n",
    "# of size 128. Using the DataStream.default_stream constructor will turn our\n",
    "# 8-bit images into floating-point decimals in [0, 1].\n",
    "stream = DataStream.default_stream(\n",
    "    train,\n",
    "    iteration_scheme=ShuffledScheme(train.num_examples, 128)\n",
    ")\n",
    "\n",
    "# Our images are of different sizes, so we'll use a Fuel transformer\n",
    "# to take random crops of size (32 x 32) from each image\n",
    "cropped_stream = RandomFixedSizeCrop(\n",
    "    stream, (32, 32), which_sources=('image_features',))\n",
    "\n",
    "# We'll use a simple MLP, so we need to flatten the images\n",
    "# from (channel, width, height) to simply (features,)\n",
    "flattened_stream = Flatten(\n",
    "    cropped_stream, which_sources=('image_features',))\n",
    "\n",
    "\n",
    "#----------------\n",
    "# Create the Theano MLP\n",
    "import theano\n",
    "from theano import tensor\n",
    "import numpy\n",
    "\n",
    "X = tensor.matrix('image_features')\n",
    "T = tensor.lmatrix('targets')\n",
    "\n",
    "W = theano.shared(\n",
    "    numpy.random.uniform(low=-0.01, high=0.01, size=(3072, 500)), 'W')\n",
    "b = theano.shared(numpy.zeros(500))\n",
    "V = theano.shared(\n",
    "    numpy.random.uniform(low=-0.01, high=0.01, size=(500, 2)), 'V')\n",
    "c = theano.shared(numpy.zeros(2))\n",
    "params = [W, b, V, c]\n",
    "\n",
    "H = tensor.nnet.sigmoid(tensor.dot(X, W) + b)\n",
    "Y = tensor.nnet.softmax(tensor.dot(H, V) + c)\n",
    "\n",
    "loss = tensor.nnet.categorical_crossentropy(Y, T.flatten()).mean()\n",
    "\n",
    "# Use Blocks to train this network\n",
    "from blocks.algorithms import GradientDescent, Scale\n",
    "from blocks.extensions import Printing\n",
    "from blocks.extensions.monitoring import TrainingDataMonitoring\n",
    "from blocks.main_loop import MainLoop\n",
    "\n",
    "algorithm = GradientDescent(cost=loss, parameters=params,\n",
    "                            step_rule=Scale(learning_rate=0.1))\n",
    "\n",
    "# We want to monitor the cost as we train\n",
    "loss.name = 'loss'\n",
    "extensions = [TrainingDataMonitoring([loss], every_n_batches=1),\n",
    "              Printing(every_n_batches=1)]\n",
    "\n",
    "main_loop = MainLoop(data_stream=flattened_stream, algorithm=algorithm,\n",
    "                     extensions=extensions)\n",
    "#main_loop.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import math\n",
    "\n",
    "import numpy\n",
    "\n",
    "from fuel.transformers import ExpectsAxisLabels, SourcewiseTransformer\n",
    "\n",
    "class BilinearRescale(SourcewiseTransformer, ExpectsAxisLabels):\n",
    "    \"\"\"Resize an image to a fixed window size. Use bilinear interpolation with 4-relative nearest neighbors.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_stream : :class:`AbstractDataStream`\n",
    "        The data stream to wrap.\n",
    "    window_shape : tuple\n",
    "        The `(height, width)` tuple representing the size of the output\n",
    "        window.\n",
    "    Notes\n",
    "    -----\n",
    "    This transformer expects to act on stream sources which provide one of\n",
    "     * Single images represented as 3-dimensional ndarrays, with layout\n",
    "       `(channel, height, width)`.\n",
    "     * Batches of images represented as lists of 3-dimensional ndarrays,\n",
    "       possibly of different shapes (i.e. images of differing\n",
    "       heights/widths).\n",
    "     * Batches of images represented as 4-dimensional ndarrays, with\n",
    "       layout `(batch, channel, height, width)`.\n",
    "    The format of the stream will be un-altered, i.e. if lists are\n",
    "    yielded by `data_stream` then lists will be yielded by this\n",
    "    transformer.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_stream, image_shape, **kwargs):\n",
    "        self.image_shape = image_shape\n",
    "        kwargs.setdefault('produces_examples', data_stream.produces_examples)\n",
    "        kwargs.setdefault('axis_labels', data_stream.axis_labels)\n",
    "        super(BilinearRescale, self).__init__(data_stream, **kwargs)\n",
    "\n",
    "    def transform_source_batch(self, source, source_name):\n",
    "        self.verify_axis_labels(('batch', 'channel', 'height', 'width'),\n",
    "                                self.data_stream.axis_labels[source_name],\n",
    "                                source_name)\n",
    "        height, width = self.image_shape\n",
    "        print(source.shape)\n",
    "        if isinstance(source, numpy.ndarray) and source.ndim == 4:\n",
    "            # Hardcoded assumption of (batch, channels, height, width).\n",
    "            # This is what the fast Cython code supports.\n",
    "            raise Exception\n",
    "        \n",
    "        elif all(isinstance(b, numpy.ndarray) and b.ndim == 3 for b in source):\n",
    "            return [self.transform_source_example(im, source_name)\n",
    "                    for im in source]\n",
    "        else:\n",
    "            raise ValueError(\"uninterpretable batch format; expected a list \"\n",
    "                             \"of arrays with ndim = 3, or an array with \"\n",
    "                             \"ndim = 4\")\n",
    "\n",
    "    def transform_source_example(self, example, source_name):\n",
    "        self.verify_axis_labels(('channel', 'height', 'width'),\n",
    "                                self.data_stream.axis_labels[source_name],\n",
    "                                source_name)\n",
    "        height, width = self.image_shape\n",
    "        nb_channel = example.shape[0] #That line could be replace by something more elegant.\n",
    "        \n",
    "        if not isinstance(example, numpy.ndarray) or example.ndim != 3:\n",
    "            raise ValueError(\"uninterpretable example format; expected \"\n",
    "                             \"ndarray with ndim = 3\")\n",
    "        image_height, image_width = example.shape[1:]\n",
    "        rescale_height, rescale_width = (image_height-1)/(height-1), (image_width-1)/(width-1)\n",
    "        \n",
    "        rescaled_image = np.zeros((nb_channel,height,width))\n",
    "        #Might do a cleaner version eventually\n",
    "        #print(example[0][0])\n",
    "        #print(example.shape)\n",
    "        \n",
    "        for i,j in product(range(height), range(width)):\n",
    "            x, y  = i*rescale_height, j * rescale_width\n",
    "            x1 = np.array([math.floor(x), math.ceil(x)],dtype=np.intp)\n",
    "            y1 = np.array([math.floor(y), math.ceil(y)],dtype=np.intp)\n",
    "            dx,dy = x - x1[0], y - y1[0]\n",
    "            del_x, del_y = x1[1]-x1[0], y1[1]-y1[0]\n",
    "            \n",
    "            #Stupid patching for float approximation induced problem\n",
    "            if x1[1] == image_height:\n",
    "                x1[1] -= 1\n",
    "            if y1[1] == image_width:\n",
    "                y1[1] -= 1\n",
    "                \n",
    "            if not x1[0] == x1[1] and not y1[0] == y1[1] :\n",
    "                for c in range(nb_channel):\n",
    "                    xy1,xy2 = example[c][np.ix_(x1,y1)]\n",
    "                    x1y1,x2y1,x1y2,x2y2 = xy1[0],xy1[1],xy2[0],xy2[1]\n",
    "                    rescaled_image[c,i,j] = (x2y1-x1y1)*dx/del_x + (x1y2 -x1y1) *dy/del_y + (x1y1 + x2y2 - x2y1 - x1y2) *dx/del_x*dy/del_y + x1y1\n",
    "            elif x1[0] == x1[1] and y1[0] == y1[1]:\n",
    "                rescaled_image[:,i,j] = example[:,i,j]\n",
    "            else:\n",
    "                if y1[0] == y1[1]:\n",
    "                    for c in range(nb_channel):\n",
    "                        x1y,x2y = example[c][np.ix_(x1,y1)]\n",
    "                        x1y,x2y = x1y[0], x1y[1]\n",
    "                        rescaled_image[c,i,j] = (x2y-x1y)*dx/del_x + x1y\n",
    "                else:\n",
    "                    for c in range(nb_channel):\n",
    "                        xy1,xy2 = example[c][np.ix_(x1,y1)]\n",
    "                        xy1,xy2 = xy1[0], xy1[1]\n",
    "                        rescaled_image[c,i,j] = (xy2 - xy1) * dy/del_y + xy1\n",
    "            \n",
    "        return rescaled_image\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s0,s1 = 50,50\n",
    "nb_hidden = 600\n",
    "\n",
    "x = BilinearRescale(stream, (s0,s1), which_sources=('image_features',))\n",
    "#    stream, (32, 32), which_sources=('image_features',))\n",
    "\n",
    "flattened_stream = Flatten(\n",
    "    x, which_sources=('image_features',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------\n",
      "BEFORE FIRST EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 0\n",
      "\t received_first_batch: False\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 0:\n",
      "\n",
      "(100,)\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 1\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 1:\n",
      "\t loss: 0.692335153836\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 1\n",
      "\t iterations_done: 1\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 1:\n",
      "\t loss: 0.692335153836\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "(100,)\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 1\n",
      "\t iterations_done: 2\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 2:\n",
      "\t loss: 0.728306277957\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 2\n",
      "\t iterations_done: 2\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 2:\n",
      "\t loss: 0.728306277957\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "(100,)\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 2\n",
      "\t iterations_done: 3\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 3:\n",
      "\t loss: 1.8418355196\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 3\n",
      "\t iterations_done: 3\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 3:\n",
      "\t loss: 1.8418355196\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "(100,)\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 3\n",
      "\t iterations_done: 4\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 4:\n",
      "\t loss: 4.99158623294\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 4\n",
      "\t iterations_done: 4\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 4:\n",
      "\t loss: 4.99158623294\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "(100,)\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 4\n",
      "\t iterations_done: 5\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 5:\n",
      "\t loss: 0.753907936171\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 5\n",
      "\t iterations_done: 5\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 5:\n",
      "\t loss: 0.753907936171\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "(100,)\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 5\n",
      "\t iterations_done: 6\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 6:\n",
      "\t loss: 0.841845443058\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 6\n",
      "\t iterations_done: 6\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 6:\n",
      "\t loss: 0.841845443058\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "(100,)\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 6\n",
      "\t iterations_done: 7\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 7:\n",
      "\t loss: 0.932844193664\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 7\n",
      "\t iterations_done: 7\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 7:\n",
      "\t loss: 0.932844193664\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "(100,)\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 7\n",
      "\t iterations_done: 8\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 8:\n",
      "\t loss: 0.961310316603\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 8\n",
      "\t iterations_done: 8\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 8:\n",
      "\t loss: 0.961310316603\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "(100,)\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 8\n",
      "\t iterations_done: 9\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 9:\n",
      "\t loss: 0.849745453851\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 9\n",
      "\t iterations_done: 9\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 9:\n",
      "\t loss: 0.849745453851\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "(100,)\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 9\n",
      "\t iterations_done: 10\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 10:\n",
      "\t loss: 0.78408144824\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 10\n",
      "\t iterations_done: 10\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 10:\n",
      "\t loss: 0.78408144824\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "(100,)\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 10\n",
      "\t iterations_done: 11\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 11:\n",
      "\t loss: 0.733336737548\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 11\n",
      "\t iterations_done: 11\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 11:\n",
      "\t loss: 0.733336737548\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "(100,)\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 11\n",
      "\t iterations_done: 12\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 12:\n",
      "\t loss: 0.711534978899\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 12\n",
      "\t iterations_done: 12\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 12:\n",
      "\t loss: 0.711534978899\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "(100,)\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 12\n",
      "\t iterations_done: 13\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 13:\n",
      "\t loss: 0.699490614927\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 13\n",
      "\t iterations_done: 13\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 13:\n",
      "\t loss: 0.699490614927\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "(100,)\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 13\n",
      "\t iterations_done: 14\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 14:\n",
      "\t loss: 0.693968648509\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 14\n",
      "\t iterations_done: 14\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 14:\n",
      "\t loss: 0.693968648509\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "(100,)\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 14\n",
      "\t iterations_done: 15\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 15:\n",
      "\t loss: 0.690998254983\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 15\n",
      "\t iterations_done: 15\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 15:\n",
      "\t loss: 0.690998254983\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "(100,)\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 15\n",
      "\t iterations_done: 16\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 16:\n",
      "\t loss: 0.689418618171\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 16\n",
      "\t iterations_done: 16\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 16:\n",
      "\t loss: 0.689418618171\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "(100,)\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 16\n",
      "\t iterations_done: 17\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 17:\n",
      "\t loss: 0.688427116113\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 17\n",
      "\t iterations_done: 17\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 17:\n",
      "\t loss: 0.688427116113\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "(100,)\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 17\n",
      "\t iterations_done: 18\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 18:\n",
      "\t loss: 0.687728719868\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 18\n",
      "\t iterations_done: 18\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 18:\n",
      "\t loss: 0.687728719868\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "(100,)\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 18\n",
      "\t iterations_done: 19\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 19:\n",
      "\t loss: 0.68715035763\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 19\n",
      "\t iterations_done: 19\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 19:\n",
      "\t loss: 0.68715035763\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "(100,)\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 19\n",
      "\t iterations_done: 20\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 20:\n",
      "\t loss: 0.686621314703\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 20\n",
      "\t iterations_done: 20\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 20:\n",
      "\t loss: 0.686621314703\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "(100,)\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 20\n",
      "\t iterations_done: 21\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 21:\n",
      "\t loss: 0.686100377771\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 21\n",
      "\t iterations_done: 21\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 21:\n",
      "\t loss: 0.686100377771\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "(100,)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:blocks.main_loop:Received epoch interrupt signal.\n",
      "\n",
      "Blocks will complete this epoch of training and run extensions before exiting. If you do not want to complete this epoch, press CTRL + C again to stop training after the current batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: True\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 21\n",
      "\t iterations_done: 22\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 22:\n",
      "\t loss: 0.685569927051\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: True\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 22\n",
      "\t iterations_done: 22\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 22:\n",
      "\t loss: 0.685569927051\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "TRAINING HAS BEEN FINISHED:\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: True\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 22\n",
      "\t iterations_done: 22\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 22:\n",
      "\t loss: 0.685569927051\n",
      "\t misclassificationrate_apply_error_rate: 1.0\n",
      "\t training_finished: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "from itertools import product\n",
    "from theano import tensor\n",
    "import numpy\n",
    "import numpy as np\n",
    "X = tensor.matrix('image_features')\n",
    "T = tensor.lmatrix('targets')\n",
    "\n",
    "W = theano.shared(\n",
    "    numpy.random.uniform(low=-0.01, high=0.01, size=(s0*s1*3, nb_hidden)), 'W')\n",
    "b = theano.shared(numpy.zeros(nb_hidden))\n",
    "V = theano.shared(\n",
    "    numpy.random.uniform(low=-0.01, high=0.01, size=(nb_hidden, 2)), 'V')\n",
    "c = theano.shared(numpy.zeros(2))\n",
    "params = [W, b, V, c]\n",
    "\n",
    "H = tensor.nnet.sigmoid(tensor.dot(X, W) + b)\n",
    "Y = tensor.nnet.softmax(tensor.dot(H, V) + c)\n",
    "\n",
    "loss = tensor.nnet.categorical_crossentropy(Y, T.flatten()).mean()\n",
    "\n",
    "# Use Blocks to train this network\n",
    "from blocks.algorithms import GradientDescent, Scale\n",
    "from blocks.extensions import Printing\n",
    "from blocks.extensions.monitoring import TrainingDataMonitoring\n",
    "from blocks.bricks.cost import CategoricalCrossEntropy,SquaredError,MisclassificationRate\n",
    "from blocks.main_loop import MainLoop\n",
    "\n",
    "\n",
    "classification_error= MisclassificationRate().apply(np.argmax(Y),T)\n",
    "\n",
    "algorithm = GradientDescent(cost=loss, parameters=params,\n",
    "                            step_rule=Scale(learning_rate=0.1))\n",
    "\n",
    "# We want to monitor the cost as we train\n",
    "loss.name = 'loss'\n",
    "extensions = [TrainingDataMonitoring([loss, classification_error], every_n_batches=1),\n",
    "              Printing(every_n_batches=1)]\n",
    "\n",
    "main_loop = MainLoop(data_stream=flattened_stream, algorithm=algorithm,\n",
    "                     extensions=extensions)\n",
    "main_loop.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(image_width,image_height)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

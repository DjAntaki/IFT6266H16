{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://en.wikipedia.org/wiki/Image_scaling#Algorithms\n",
    "\n",
    "to do :\n",
    "- own validation set\n",
    "- resize images transformer (Bilinear done?)\n",
    "- make noise transformer\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#I don't think its working yet.\n",
    "from __future__ import division\n",
    "import math\n",
    "from fractions import Fraction\n",
    "\n",
    "from fuel.transformers import ExpectsAxisLabels, SourcewiseTransformer\n",
    "\n",
    "class BilinearRescale(SourcewiseTransformer, ExpectsAxisLabels):\n",
    "    \"\"\"Resize an image to a fixed window size. Use bilinear interpolation with 4-relative nearest neighbors.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_stream : :class:`AbstractDataStream`\n",
    "        The data stream to wrap.\n",
    "    window_shape : tuple\n",
    "        The `(height, width)` tuple representing the size of the output\n",
    "        window.\n",
    "    Notes\n",
    "    -----\n",
    "    This transformer expects to act on stream sources which provide one of\n",
    "     * Single images represented as 3-dimensional ndarrays, with layout\n",
    "       `(channel, height, width)`.\n",
    "     * Batches of images represented as lists of 3-dimensional ndarrays,\n",
    "       possibly of different shapes (i.e. images of differing\n",
    "       heights/widths).\n",
    "     * Batches of images represented as 4-dimensional ndarrays, with\n",
    "       layout `(batch, channel, height, width)`.\n",
    "    The format of the stream will be un-altered, i.e. if lists are\n",
    "    yielded by `data_stream` then lists will be yielded by this\n",
    "    transformer.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_stream, image_shape, **kwargs):\n",
    "        self.image_shape = image_shape\n",
    "        kwargs.setdefault('produces_examples', data_stream.produces_examples)\n",
    "        kwargs.setdefault('axis_labels', data_stream.axis_labels)\n",
    "        super(BilinearRescale, self).__init__(data_stream, **kwargs)\n",
    "\n",
    "    def transform_source_batch(self, source, source_name):\n",
    "        self.verify_axis_labels(('batch', 'channel', 'height', 'width'),\n",
    "                                self.data_stream.axis_labels[source_name],\n",
    "                                source_name)\n",
    "        height, width = self.image_shape\n",
    "        print(source.shape)\n",
    "        if isinstance(source, np.ndarray) and source.ndim == 4:\n",
    "            # Not yet supported(batch, channels, height, width).\n",
    "            raise Exception\n",
    "        \n",
    "        elif all(isinstance(b, np.ndarray) and b.ndim == 3 for b in source):\n",
    "            return [self.transform_source_example(im, source_name)\n",
    "                    for im in source]\n",
    "        else:\n",
    "            raise ValueError(\"uninterpretable batch format; expected a list \"\n",
    "                             \"of arrays with ndim = 3, or an array with \"\n",
    "                             \"ndim = 4\")\n",
    "\n",
    "    def transform_source_example(self, example, source_name):\n",
    "        self.verify_axis_labels(('channel', 'height', 'width'),\n",
    "                                self.data_stream.axis_labels[source_name],\n",
    "                                source_name)\n",
    "        height, width = self.image_shape\n",
    "        nb_channel = example.shape[0] #That line could be replace by something more elegant.\n",
    "        \n",
    "        if not isinstance(example, np.ndarray) or example.ndim != 3:\n",
    "            raise ValueError(\"uninterpretable example format; expected \"\n",
    "                             \"ndarray with ndim = 3\")\n",
    "        image_height, image_width = example.shape[1:]\n",
    "        #rescale_height, rescale_width = (image_height-1)/(height-1), (image_width-1)/(width-1)\n",
    "        rescale_height, rescale_width = Fraction(image_height-1,height-1), Fraction(image_width-1,width-1)\n",
    "        \n",
    "        rescaled_image = np.zeros((nb_channel,height,width))\n",
    "        #That code ain't pretty. Might do a cleaner version eventually\n",
    "        \n",
    "        for i,j in product(range(height), range(width)):\n",
    "            x, y  = i*rescale_height, j * rescale_width\n",
    "            x1 = np.array([math.floor(x), math.ceil(x)],dtype=np.intp)\n",
    "            y1 = np.array([math.floor(y), math.ceil(y)],dtype=np.intp)\n",
    "            dx,dy = x - x1[0], y - y1[0]\n",
    "            del_x, del_y = x1[1]-x1[0], y1[1]-y1[0]\n",
    "            \n",
    "            #Stupid patching for float approximation induced problem\n",
    "            if x1[1] == image_height:\n",
    "                x1[1] -= 1\n",
    "            if y1[1] == image_width:\n",
    "                y1[1] -= 1\n",
    "                \n",
    "            if not x1[0] == x1[1] and not y1[0] == y1[1] :\n",
    "                for c in range(nb_channel):\n",
    "                    xy1,xy2 = example[c][np.ix_(x1,y1)]\n",
    "                    x1y1,x2y1,x1y2,x2y2 = xy1[0],xy1[1],xy2[0],xy2[1]\n",
    "                    rescaled_image[c,i,j] = (x2y1-x1y1)*dx/del_x + (x1y2 -x1y1) *dy/del_y + (x1y1 + x2y2 - x2y1 - x1y2) *dx/del_x*dy/del_y + x1y1\n",
    "            elif x1[0] == x1[1] and y1[0] == y1[1]:\n",
    "                rescaled_image[:,i,j] = example[:,x1[0],y1[0]]\n",
    "            else:\n",
    "                if y1[0] == y1[1]:\n",
    "                    for c in range(nb_channel):\n",
    "                        x1y,x2y = example[c][np.ix_(x1,y1)]\n",
    "                        x1y,x2y = x1y[0], x1y[1]\n",
    "                        rescaled_image[c,i,j] = (x2y-x1y)*dx/del_x + x1y\n",
    "                else:\n",
    "                    for c in range(nb_channel):\n",
    "                        xy1,xy2 = example[c][np.ix_(x1,y1)]\n",
    "                        xy1,xy2 = xy1[0], xy1[1]\n",
    "                        rescaled_image[c,i,j] = (xy2 - xy1) * dy/del_y + xy1\n",
    "            \n",
    "        return rescaled_image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This section deals with the dogs_vs_cats problem with a simple MLP. It's mainly for testing the BilinearRescale\n",
    "\n",
    "s0,s1 = 100,100\n",
    "nb_hidden = 600\n",
    "num_epochs = 1\n",
    "batch_size = 1\n",
    "num_batches = 25\n",
    "\n",
    "# Let's load and process the dataset\n",
    "import numpy as np\n",
    "from fuel.datasets.dogs_vs_cats import DogsVsCats\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import ShuffledScheme\n",
    "from fuel.transformers.image import RandomFixedSizeCrop\n",
    "from fuel.transformers import Flatten\n",
    "\n",
    "train = DogsVsCats(('train',), subset=slice(0, 20000))\n",
    "test = DogsVsCats(('test',), subset=slice(0,100))\n",
    "\n",
    "train_stream = DataStream.default_stream(\n",
    "    train,\n",
    "    iteration_scheme=ShuffledScheme(train.num_examples, batch_size)\n",
    ")\n",
    "\n",
    "test_stream = DataStream.default_stream(\n",
    "    test,\n",
    "    iteration_scheme=ShuffledScheme(test.num_examples, batch_size)\n",
    ")\n",
    "\n",
    "train_stream = BilinearRescale(train_stream, (s0,s1), which_sources=('image_features',))\n",
    "test_stream = BilinearRescale(test_stream, (s0,s1), which_sources=('image_features',))\n",
    "\n",
    "if True:\n",
    "    train_stream = Flatten(train_stream, which_sources=('image_features',))\n",
    "    test_stream = Flatten(test_stream, which_sources=('image_features',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "from itertools import product\n",
    "from theano import tensor\n",
    "    \n",
    "X = tensor.matrix('image_features')\n",
    "T = tensor.lmatrix('targets')\n",
    "W = theano.shared(\n",
    "    np.random.uniform(low=-0.01, high=0.01, size=(s0*s1*3, nb_hidden)), 'W')\n",
    "b = theano.shared(np.zeros(nb_hidden))\n",
    "V = theano.shared(\n",
    "    np.random.uniform(low=-0.01, high=0.01, size=(nb_hidden, 2)), 'V')\n",
    "c = theano.shared(np.zeros(2))\n",
    "params = [W, b, V, c]\n",
    "\n",
    "H = tensor.nnet.sigmoid(tensor.dot(X, W) + b)\n",
    "Y = tensor.nnet.softmax(tensor.dot(H, V) + c)\n",
    "\n",
    "# Use Blocks to train this network\n",
    "from blocks.algorithms import GradientDescent, Scale\n",
    "from blocks.extensions import Printing\n",
    "from blocks.extensions import FinishAfter, Timing, Printing, ProgressBar\n",
    "from blocks.extensions.monitoring import (DataStreamMonitoring,\n",
    "                                          TrainingDataMonitoring)\n",
    "from blocks.bricks.cost import CategoricalCrossEntropy,MisclassificationRate\n",
    "from blocks.monitoring import aggregation\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.model import Model \n",
    "\n",
    "loss = tensor.nnet.categorical_crossentropy(Y, T.flatten()).mean()\n",
    "\n",
    "#classification_error= MisclassificationRate().apply(tensor.argmax(Y),T)\n",
    "classification_error= MisclassificationRate().apply(T.flatten(),Y)\n",
    "#squared_error = SquaredError().apply(Y.T,tensor.eye(10)[T])\n",
    "\n",
    "algorithm = GradientDescent(cost=loss, parameters=params,\n",
    "                            step_rule=Scale(learning_rate=0.1))\n",
    "\n",
    "# We want to monitor the cost as we train\n",
    "loss.name = 'loss'\n",
    "extensions = [Timing(), FinishAfter(after_n_epochs=num_epochs,\n",
    "                              after_n_batches=num_batches),\n",
    "              TrainingDataMonitoring([loss,classification_error],prefix=\"train\",after_epoch=True),    \n",
    "            #  DataStreamMonitoring(\n",
    "           #           [loss],#classification_error\n",
    "          #            test_stream,prefix=\"test\"),\n",
    "                  #ProgressBar(),\n",
    "                 Printing(every_n_batches=1)]\n",
    "\n",
    "#extensions = [Timing(),\n",
    "#              FinishAfter(after_n_epochs=num_epochs),\n",
    "#              TrainingDataMonitoring([loss,classification_error],prefix=\"train\",after_epoch=True),\n",
    "             #   DataStreamMonitoring([loss, classification_error],test_stream,prefix=\"test\"),\n",
    "             #    ProgressBar(),\n",
    "#                  Printing() ]\n",
    "\n",
    "#TrainingDataMonitoring([loss, classification_error,squared_error,aggregation.mean(algorithm.total_gradient_norm)], \n",
    "      #                  every_n_batches=5,\n",
    "       #               prefix=\"train\",\n",
    "        #              after_epoch=True)\n",
    "#model = Model(loss)\n",
    "\n",
    "main_loop = MainLoop(data_stream=train_stream, algorithm=algorithm,\n",
    "                     extensions=extensions)\n",
    "main_loop.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "I was going try coding a CNN, but then i figured that i could probably reuse the LeNet example in blocks-example\n",
    "\n",
    "Link to the depot : https://github.com/mila-udem/blocks-examples/blob/master/mnist_lenet/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from theano import tensor\n",
    "\n",
    "from blocks.algorithms import GradientDescent, Scale\n",
    "from blocks.bricks import (Rectifier, Initializable,\n",
    "                           Softmax)\n",
    "from blocks.bricks.cost import CategoricalCrossEntropy, MisclassificationRate\n",
    "from blocks.extensions import FinishAfter, Timing, Printing, ProgressBar\n",
    "from blocks.extensions.monitoring import (DataStreamMonitoring,\n",
    "                                          TrainingDataMonitoring)\n",
    "from blocks.extensions.saveload import Checkpoint\n",
    "from blocks.graph import ComputationGraph\n",
    "from blocks.initialization import Constant, Uniform\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.model import Model\n",
    "from blocks.monitoring import aggregation\n",
    "from fuel.schemes import ShuffledScheme\n",
    "from fuel.streams import DataStream\n",
    "from toolz.itertoolz import interleave\n",
    "from itertools import product\n",
    "    \n",
    "from LeNet import LeNet\n",
    "\n",
    "def main_convnet(save_to, num_epochs, train, test, input_size=None, feature_maps=None, mlp_hiddens=None,\n",
    "         conv_sizes=None, pool_sizes=None, batch_size=50, num_batches=None):\n",
    "    if feature_maps is None:\n",
    "        feature_maps = [20, 50]\n",
    "    if mlp_hiddens is None:\n",
    "        mlp_hiddens = [500]\n",
    "    if conv_sizes is None:\n",
    "        conv_sizes = [5, 5]\n",
    "    if pool_sizes is None:\n",
    "        pool_sizes = [2, 2]\n",
    "    if input_size is None :\n",
    "        input_size = (150, 150)\n",
    "    output_size = 2\n",
    "    \n",
    "    # Use ReLUs everywhere and softmax for the final prediction\n",
    "    conv_activations = [Rectifier() for _ in feature_maps]\n",
    "    mlp_activations = [Rectifier() for _ in mlp_hiddens] + [Softmax()]\n",
    "    convnet = LeNet(conv_activations, 3, input_size,\n",
    "                    filter_sizes=zip(conv_sizes, conv_sizes),\n",
    "                    feature_maps=feature_maps,\n",
    "                    pooling_sizes=zip(pool_sizes, pool_sizes),\n",
    "                    top_mlp_activations=mlp_activations,\n",
    "                    top_mlp_dims=mlp_hiddens + [output_size],\n",
    "                    border_mode='full',\n",
    "                    weights_init=Uniform(width=.2),\n",
    "                    biases_init=Constant(0))\n",
    "    \n",
    "    \n",
    "    # We push initialization config to set different initialization schemes\n",
    "    # for convolutional layers.\n",
    "    convnet.push_initialization_config()\n",
    "    convnet.layers[0].weights_init = Uniform(width=.2)\n",
    "    convnet.layers[1].weights_init = Uniform(width=.09)\n",
    "    convnet.top_mlp.linear_transformations[0].weights_init = Uniform(width=.08)\n",
    "    convnet.top_mlp.linear_transformations[1].weights_init = Uniform(width=.11)\n",
    "    convnet.initialize()\n",
    "    logging.info(\"Input dim: {} {} {}\".format(\n",
    "        *convnet.children[0].get_dim('input_')))\n",
    "    for i, layer in enumerate(convnet.layers):\n",
    "        logging.info(\"Layer {} ({}) dim: {} {} {}\".format(\n",
    "            i, layer.__class__.__name__, *layer.get_dim('output')))\n",
    "\n",
    "    x = tensor.tensor4('image_features')\n",
    "    y = tensor.lmatrix('targets')\n",
    "\n",
    "    # Normalize input and apply the convnet\n",
    "    probs = convnet.apply(x)\n",
    "    cost = CategoricalCrossEntropy().apply(y.flatten(),\n",
    "            probs).copy(name='cost')\n",
    "    error_rate = MisclassificationRate().apply(y.flatten(), probs).copy(\n",
    "            name='error_rate')\n",
    "\n",
    "    cg = ComputationGraph([cost, error_rate]) #2 cost?\n",
    "\n",
    "    \n",
    "    #Generating stream\n",
    "    train_stream = DataStream.default_stream(\n",
    "        train,\n",
    "        iteration_scheme=ShuffledScheme(train.num_examples, batch_size)\n",
    "    )\n",
    "\n",
    "    test_stream = DataStream.default_stream(\n",
    "        test,\n",
    "        iteration_scheme=ShuffledScheme(test.num_examples, batch_size)\n",
    "    )\n",
    "    \n",
    "    train_stream = BilinearRescale(train_stream, input_size, which_sources=('image_features',))\n",
    "    test_stream = BilinearRescale(test_stream, input_size, which_sources=('image_features',))\n",
    "\n",
    "    # Train with simple SGD\n",
    "    algorithm = GradientDescent(\n",
    "        cost=cost, parameters=cg.parameters,\n",
    "        step_rule=Scale(learning_rate=0.1))\n",
    "    # `Timing` extension reports time for reading data, aggregating a batch\n",
    "    # and monitoring;\n",
    "    # `ProgressBar` displays a nice progress bar during training.\n",
    "    extensions = [Timing(),\n",
    "                  FinishAfter(after_n_epochs=num_epochs,\n",
    "                              after_n_batches=num_batches),\n",
    "                  TrainingDataMonitoring(\n",
    "                      [cost, error_rate,\n",
    "                       aggregation.mean(algorithm.total_gradient_norm)],\n",
    "                      prefix=\"train\",\n",
    "                      every_n_batches=1),\n",
    "                  Checkpoint(save_to),\n",
    "                  ProgressBar(),\n",
    "                  Printing(every_n_batches=1)]\n",
    "\n",
    "    model = Model(cost)\n",
    "\n",
    "    main_loop = MainLoop(\n",
    "        algorithm,\n",
    "        train_stream,\n",
    "        model=model,\n",
    "        extensions=extensions)\n",
    "\n",
    "    main_loop.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's load and process the dataset\n",
    "import numpy as np\n",
    "from fuel.datasets.dogs_vs_cats import DogsVsCats\n",
    "\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import ShuffledScheme\n",
    "from fuel.transformers.image import RandomFixedSizeCrop\n",
    "from fuel.transformers import Flatten\n",
    "\n",
    "# Load the training set\n",
    "train = DogsVsCats(('train',), subset=slice(0, 200))\n",
    "test = DogsVsCats(('test',), subset=slice(0,2000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'BilinearRescale' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-135ecc00fcd8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m main_convnet(\"lenetsave.txt\", 1, train, test, input_size=(150,150), feature_maps=None, mlp_hiddens=None,\n\u001b[1;32m----> 4\u001b[1;33m          conv_sizes=None, pool_sizes=None, batch_size=1,num_batches=10)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-2236fce377f7>\u001b[0m in \u001b[0;36mmain_convnet\u001b[1;34m(save_to, num_epochs, train, test, input_size, feature_maps, mlp_hiddens, conv_sizes, pool_sizes, batch_size, num_batches)\u001b[0m\n\u001b[0;32m     91\u001b[0m     )\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m     \u001b[0mtrain_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBilinearRescale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_stream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhich_sources\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'image_features'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m     \u001b[0mtest_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBilinearRescale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_stream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhich_sources\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'image_features'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'BilinearRescale' is not defined"
     ]
    }
   ],
   "source": [
    "#main_convnet(\"lenetsave.txt\",, batch_size=8, num_batches=5)\n",
    "\n",
    "main_convnet(\"lenetsave.txt\", 1, train, test, input_size=(150,150), feature_maps=None, mlp_hiddens=None,\n",
    "         conv_sizes=None, pool_sizes=None, batch_size=1,num_batches=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%debug #Fucking hell ce bug. Jy comprends rien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import theano\n",
    "from itertools import product\n",
    "from blocks.graph import ComputationGraph\n",
    "# Use Blocks to train this network\n",
    "from blocks.algorithms import GradientDescent, Scale\n",
    "from blocks.extensions import Printing\n",
    "from blocks.extensions import FinishAfter, Timing, Printing, ProgressBar\n",
    "from blocks.extensions.monitoring import (DataStreamMonitoring,\n",
    "                                          TrainingDataMonitoring)\n",
    "from blocks.bricks.cost import CategoricalCrossEntropy,MisclassificationRate\n",
    "from blocks.monitoring import aggregation\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.model import Model \n",
    "\n",
    "def main(save_to, model, num_epochs, train, test, input_size = (150,150), batch_size=50, learning_rate=0.05,flatten_stream=False,\n",
    "         num_batches=10):\n",
    "\n",
    "    #Variables theano\n",
    "    x = tensor.matrix('image_features')\n",
    "    T = tensor.lmatrix(name='targets')\n",
    "    Y = mlp.apply(x)\n",
    "\n",
    "    model.push_initialization_config()\n",
    "    #model.children[0].weights_init = Constant(0.01)\n",
    "    model.initialize()\n",
    "\n",
    "    #Cost\n",
    "#    loss = tensor.nnet.categorical_crossentropy(Y, T.flatten()).mean()\n",
    "    #classification_error= MisclassificationRate().apply(tensor.argmax(Y),T)\n",
    "    classification_error= MisclassificationRate().apply(T.flatten(),Y)\n",
    "    #squared_error = SquaredError().apply(Y.T,tensor.eye(10)[T])\n",
    "    cost = CategoricalCrossEntropy().apply(T.flatten(),Y)\n",
    "    cg = ComputationGraph(cost)\n",
    "    algorithm = GradientDescent(cost=cost, parameters=cg.parameters,\n",
    "                             step_rule=Scale(learning_rate=learning_rate))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    #Generating stream\n",
    "    train_stream = DataStream.default_stream(\n",
    "        train,\n",
    "        iteration_scheme=ShuffledScheme(train.num_examples, batch_size)\n",
    "    )\n",
    "\n",
    "    test_stream = DataStream.default_stream(\n",
    "        test,\n",
    "        iteration_scheme=ShuffledScheme(test.num_examples, batch_size)\n",
    "    )\n",
    "\n",
    "\n",
    "    #Rescale images\n",
    "    train_stream = BilinearRescale(train_stream, input_size, which_sources=('image_features',))\n",
    "    test_stream = BilinearRescale(test_stream, input_size, which_sources=('image_features',)) \n",
    "    \n",
    "    #Flattening the stream\n",
    "    if flatten_stream is True:\n",
    "        train_stream = Flatten(train_stream, which_sources=('image_features',))\n",
    "        test_stream = Flatten(test_stream, which_sources=('image_features',))\n",
    "\n",
    "        \n",
    "    #Monitoring\n",
    "    cost.name = 'cost'\n",
    "    extensions = [Timing(), FinishAfter(after_n_epochs=num_epochs,\n",
    "                                  after_n_batches=num_batches),\n",
    "                        TrainingDataMonitoring([cost,classification_error],prefix=\"train\"),    \n",
    "\n",
    "                      DataStreamMonitoring([cost, classification_error], test_stream,prefix=\"test\"),\n",
    "                      #ProgressBar(),\n",
    "                     Printing(every_n_batches=1)]\n",
    "\n",
    "    #TrainingDataMonitoring([loss, classification_error,squared_error,aggregation.mean(algorithm.total_gradient_norm)], \n",
    "          #                  every_n_batches=5,\n",
    "           #               prefix=\"train\",\n",
    "            #              after_epoch=True)\n",
    "    #model = Model(loss)\n",
    "\n",
    "    main_loop = MainLoop(data_stream=train_stream, algorithm=algorithm,\n",
    "                         extensions=extensions)\n",
    "    main_loop.run()\n",
    "\n",
    "\n",
    "#Model\n",
    "from blocks.bricks import Tanh,Linear, Rectifier, Softmax, MLP, Logistic\n",
    "from blocks.initialization import IsotropicGaussian, Constant\n",
    "from theano import tensor\n",
    "\n",
    "input_size = (150,150)\n",
    "\n",
    "mlp = MLP(activations=[Rectifier(name='rect0'),Logistic(name='sigmoid_1'),Softmax(name='softmax_2')], dims=[input_size[0]*input_size[1]*3, 1000, 500, 2],\n",
    "          weights_init=IsotropicGaussian(), biases_init=Constant(0.01))\n",
    "\n",
    "main('mlp1.txt',mlp,2,train,test,input_size = input_size, batch_size=1, flatten_stream=True, num_batches=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cuda lib_dirs error (it isnt in lib_dirs) cuda shared libra\n",
    "LD_LIBRAIRY_\n",
    "- Wierd import error with LeNet.\n",
    "- retrieve information from test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
